{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from utilities import pickleFixLoad\n",
    "from DCS import *\n",
    "from sentences import *\n",
    "from romtoslp import rom_slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SentencePreprocess(sentenceObj):\n",
    "    \"\"\"\n",
    "    Considering word names only\n",
    "    ***{Word forms or cngs can also be used}\n",
    "    \"\"\"\n",
    "    chunkDict = {}\n",
    "    wordList = []\n",
    "    cngList = []\n",
    "    revMap2Chunk = []\n",
    "    qu = []\n",
    "\n",
    "    cid = -1\n",
    "    for chunk in sentenceObj.chunk:\n",
    "        # print()\n",
    "        cid = cid+1\n",
    "        chunkDict[cid] = {}\n",
    "        canBeQuery = 0\n",
    "        if len(chunk.chunk_words.keys()) == 1:\n",
    "            canBeQuery = 1 # Unsegmentable Chunk\n",
    "        for pos in chunk.chunk_words.keys():\n",
    "            chunkDict[cid][pos] = []\n",
    "            if(canBeQuery == 1) and (len(chunk.chunk_words[pos]) == 1):\n",
    "                canBeQuery = 2 # No cng alternative for the word\n",
    "            for word_sense in chunk.chunk_words[pos]:\n",
    "                if(len(word_sense.lemmas) > 0):\n",
    "                    wordList.append(rom_slp(word_sense.lemmas[0]))\n",
    "                    for form, config in word_sense.forms[0].items():                        \n",
    "                        cngList.append(wordTypeCheck(form, config[0]))\n",
    "                        break\n",
    "                    \n",
    "                    k = len(wordList) - 1\n",
    "                    chunkDict[cid][pos].append(k)\n",
    "                    revMap2Chunk.append((cid, pos))\n",
    "                    if canBeQuery == 2:\n",
    "                        # The word has a lemma available - in some pickle file it's not\n",
    "                        # Make this word query\n",
    "                        qu.append(k)\n",
    "    return (chunkDict, wordList, revMap2Chunk, qu, cngList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dcsObj = pickleFixLoad('../Text Segmentation/DCS_pick/3.p')\n",
    "sentenceObj = pickleFixLoad('../TextSegmentation/Pickles//3.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vajra', 'ca', 'ca', 'mOktka', 'ca', '', 'eva', 'mÄ\\x81á¹\\x87kya', 'nÄ«la', 'eva', 'ca', 'ca']\n",
      "[0, 3, 7, 8, 9]\n",
      "[69, 2, 2, 71, 2, -44, 2, 71, 69, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "(chunkDict, wordList, revMap2Chunk, qu, cngList) = SentencePreprocess(sentenceObj)\n",
    "\n",
    "from pprint import pprint\n",
    "# pprint(chunkDict)\n",
    "print(wordList)\n",
    "# pprint(revMap2Chunk)\n",
    "print(qu)\n",
    "print(cngList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vajraM ca mOktikaM cEva mARikyaM nIlam eva ca   \n",
      "[['vajra'], ['ca'], ['mauktika'], ['ca', 'eva'], ['mÄ\\x81á¹\\x87ikya'], ['nÄ«la'], ['eva'], ['ca']]\n",
      "[['31'], ['1'], ['31'], ['1', '1'], ['31'], ['31'], ['1'], ['1']]\n"
     ]
    }
   ],
   "source": [
    "SeeDCS(dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vajraM ca mOktikaM cEva mARikyaM nIlam eva ca   \n",
      "Analyzing  vajram\n",
      "0 :  vajram ['vajra'] [{'noun': ['acc. sg. m.', 'acc. sg. n.', 'nom. sg. n.']}]\n",
      "Analyzing  ca\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "Analyzing  mOktikam\n",
      "0 :  mOktikam ['mOktka'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "Analyzing  cEva\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "1 :  Eva [''] [{'verb': ['impft. [2] ac. du. 1']}]\n",
      "1 :  eva ['eva'] [{'indeclinable': ['prep.']}]\n",
      "Analyzing  mARikyam\n",
      "0 :  mÄá¹ikyam ['mÄ\\x81á¹\\x87kya'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "Analyzing  nIlam\n",
      "0 :  nÄ«lam ['nÄ«la'] [{'noun': ['acc. sg. m.', 'acc. sg. n.', 'nom. sg. n.']}]\n",
      "Analyzing  eva\n",
      "0 :  eva ['eva'] [{'indeclinable': ['prep.']}]\n",
      "Analyzing  ca\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n"
     ]
    }
   ],
   "source": [
    "SeeSentence(sentenceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullCo_ocMat = pickle.load(open('extras/all_dcs_lemmas_matrix.p', 'rb'))\n",
    "word2IndexDict = pickle.load(open('dcsLemma2index.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCo_occurMat(wordList, fullCo_ocMat, word2IndexDict):\n",
    "    nodeCount = len(wordList)\n",
    "    wordIndexList = [-1]*nodeCount\n",
    "    i = -1\n",
    "    for w in wordList:\n",
    "        i += 1\n",
    "        try:\n",
    "            wordIndexList[i] = word2IndexDict[w]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    TransitionMat = np.zeros((nodeCount, nodeCount))\n",
    "    \n",
    "    \"\"\"\n",
    "    FIXME:\n",
    "    1. HOW TO DO SMOOTHING?\n",
    "    2. HOW TO CONVERT WORD2VEC SIM. TO PROB.\n",
    "    \"\"\"\n",
    "    \n",
    "    for row in range(nodeCount):\n",
    "        for col in range(nodeCount):\n",
    "            if row != col:\n",
    "                try:\n",
    "                    TransitionMat[row][col] = fullCo_ocMat[wordIndexList[row]][wordIndexList[col]]\n",
    "                except KeyError:\n",
    "                    TransitionMat[row][col] = 0 #WHAT TO DO HERE??\n",
    "            else:\n",
    "                TransitionMat[row][col] = 0\n",
    "        \n",
    "        row_sum = np.sum(TransitionMat[row, :])\n",
    "        if(row_sum > 0):\n",
    "            TransitionMat[row, :] /= row_sum\n",
    "        else:\n",
    "            TransitionMat[row, :] = 1/(nodeCount - 1)\n",
    "        \n",
    "        TransitionMat[row, row] = 0\n",
    "        # print((TransitionMat[row, :]))\n",
    "    # MakeRowStochastic(TransitionMat)\n",
    "    return TransitionMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "co_ocMat = getCo_occurMat(wordList, fullCo_ocMat, word2IndexDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=66936, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from utilities import printProgress, validatePickleName, pickleFixLoad\n",
    "model_cbow = pickleFixLoad('extras/modelpickle10.p')\n",
    "print(model_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0557379937173\n",
      "0.030479256073288212\n"
     ]
    }
   ],
   "source": [
    "print(model_cbow.similarity('sam', 'tad'))\n",
    "print(fullCo_ocMat[word2IndexDict['sam']][word2IndexDict['tad']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullCo_ocMat_counts = pickle.load(open('extras/all_dcs_lemmas_matrix_countonly.p', 'rb'))\n",
    "unigram_counts = pickle.load(open('extras/counts_of_uniq_lemmas.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944739.0\n",
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66914"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fullCo_ocMat_counts[word2IndexDict['sam']][word2IndexDict['tad']])\n",
    "print(unigram_counts[word2IndexDict['sam']])\n",
    "len(fullCo_ocMat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51640"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2IndexDict['sam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cng_ordered_list = pickle.load(open('extras/list_of_uniq_cngs.p', 'rb'))\n",
    "cng2cngFullMat = np.mat(pickleFixLoad('extras/all_dcs_cngs_matrix_countonly.p'))\n",
    "cng_uni_count = pickle.load(open('extras/counts_of_uniq_cngs.p', 'rb'))\n",
    "\n",
    "cng2index_dict = pickle.load(open('cng2index_dict.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 46, 46, 258, 46, 25, 46, 258, 168, 46, 46, 46]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.33101129e-01,   1.33101129e-01,\n",
       "          3.41404102e-02,   1.33101129e-01,   1.12793743e-05,\n",
       "          1.33101129e-01,   3.41404102e-02,   0.00000000e+00,\n",
       "          1.33101129e-01,   1.33101129e-01,   1.33101129e-01],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.67501360e-02,   1.32355369e-01,   1.32355369e-01,\n",
       "          0.00000000e+00,   1.32355369e-01,   1.21415805e-05,\n",
       "          1.32355369e-01,   0.00000000e+00,   3.67501360e-02,\n",
       "          1.32355369e-01,   1.32355369e-01,   1.32355369e-01],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  4.00000000e-02,   1.20000000e-01,   1.20000000e-01,\n",
       "          4.00000000e-02,   1.20000000e-01,   0.00000000e+00,\n",
       "          1.20000000e-01,   4.00000000e-02,   4.00000000e-02,\n",
       "          1.20000000e-01,   1.20000000e-01,   1.20000000e-01],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.67501360e-02,   1.32355369e-01,   1.32355369e-01,\n",
       "          0.00000000e+00,   1.32355369e-01,   1.21415805e-05,\n",
       "          1.32355369e-01,   0.00000000e+00,   3.67501360e-02,\n",
       "          1.32355369e-01,   1.32355369e-01,   1.32355369e-01],\n",
       "       [  0.00000000e+00,   1.33101129e-01,   1.33101129e-01,\n",
       "          3.41404102e-02,   1.33101129e-01,   1.12793743e-05,\n",
       "          1.33101129e-01,   3.41404102e-02,   0.00000000e+00,\n",
       "          1.33101129e-01,   1.33101129e-01,   1.33101129e-01],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.59887503e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.40079461e-01,   0.00000000e+00,   6.60708544e-05,\n",
       "          0.00000000e+00,   2.40079461e-01,   2.59887503e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cng2cng_mat(cng2cngFullMat, cngList, cng2index_dict):\n",
    "    nodeCount = len(cngList)\n",
    "    cngIndexList = list(map(lambda x:cng2index_dict[str(x)], cngList))\n",
    "    print(cngIndexList)\n",
    "    TransitionMat = np.zeros((nodeCount, nodeCount))\n",
    "    \n",
    "    \"\"\"\n",
    "    FIXME:\n",
    "    1. HOW TO DO SMOOTHING?\n",
    "    2. HOW TO CONVERT WORD2VEC SIM. TO PROB.\n",
    "    \"\"\"\n",
    "    \n",
    "    for row in range(nodeCount):\n",
    "        for col in range(nodeCount):\n",
    "            if row != col:\n",
    "                try:\n",
    "#                     print(cngIndexList[row])\n",
    "                    TransitionMat[row][col] = cng2cngFullMat[cngIndexList[row],cngIndexList[col]]\n",
    "                except KeyError:\n",
    "                    TransitionMat[row][col] = 0 #WHAT TO DO HERE??\n",
    "            else:\n",
    "                TransitionMat[row][col] = 0\n",
    "        \n",
    "        row_sum = np.sum(TransitionMat[row, :])\n",
    "        if(row_sum > 0):\n",
    "            TransitionMat[row, :] /= row_sum\n",
    "        else:\n",
    "            TransitionMat[row, :] = 1/(nodeCount - 1)\n",
    "        \n",
    "        TransitionMat[row, row] = 0\n",
    "        # print((TransitionMat[row, :]))\n",
    "    # MakeRowStochastic(TransitionMat)\n",
    "    return TransitionMat\n",
    "\n",
    "get_cng2cng_mat(cng2cngFullMat, cngList, cng2index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cng2cngFullMat[2,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceObj = pickleFixLoad('../TextSegmentation/Pickles/3.p')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
