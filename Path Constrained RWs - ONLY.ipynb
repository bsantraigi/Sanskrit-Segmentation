{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader Started[Prob]...\n",
      "Dataloader Finished[Prob]...\n",
      "Preprocessing PCRW Database...\n",
      "Preprocessing PCRW Database [COMPLETE]...\n"
     ]
    }
   ],
   "source": [
    "from SktWsegRWR_utf8 import *\n",
    "import pickle\n",
    "import ProbData\n",
    "from ProbModels import *\n",
    "import multiprocessing\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "import csv\n",
    "from utilities import *\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# LOAD AND PREPROCESS MATRICES\n",
    "#============================================================\n",
    "# DO THIS IN PROBDATA_2\n",
    "# mat_lem2cng = json.load(open('../NewData/data1.json', 'r'))\n",
    "# mat_cng2lem = json.load(open('../NewData/data2.json', 'r'))\n",
    "# mat_tup2cng = json.load(open('../NewData/data3.json', 'r'))\n",
    "# mat_tup2lem = json.load(open('../NewData/data4.json', 'r'))\n",
    "\n",
    "mat_lem2cng = pickle.load(open('../NewData/mat_lem2cng.p', 'rb'), encoding='utf-8')\n",
    "mat_cng2lem = pickle.load(open('../NewData/mat_cng2lem.p', 'rb'), encoding='utf-8')\n",
    "mat_tup2cng = pickle.load(open('../NewData/mat_tup2cng.p', 'rb'), encoding='utf-8')\n",
    "mat_tup2lem = pickle.load(open('../NewData/mat_tup2lem.p', 'rb'), encoding='utf-8')\n",
    "mat_selfLemCng_evidence = pickle.load(open('../NewData/mat_selfLemCng_evidence.p', 'rb'), encoding='utf-8')\n",
    "mat_selfLemCLASS_evidence = pickle.load(open('../NewData/mat_selfLemCLASS_evidence.p', 'rb'), encoding='utf-8')\n",
    "\n",
    "# Get count of each key in the matrices\n",
    "mat_lem2cng_1D = {}\n",
    "for lem in mat_lem2cng.keys():\n",
    "    mainset = []\n",
    "    for fs in mat_lem2cng[lem].values():\n",
    "         mainset.extend(fs)\n",
    "    mainset = set(mainset)\n",
    "    mat_lem2cng_1D[lem] = len(mainset)\n",
    "    \n",
    "\n",
    "mat_cng2lem_1D = {}\n",
    "for cng in mat_cng2lem.keys():\n",
    "    mainset = []\n",
    "    for fs in mat_cng2lem[cng].values():\n",
    "         mainset.extend(fs)\n",
    "    mainset = set(mainset)\n",
    "    mat_cng2lem_1D[cng] = len(mainset)\n",
    "    \n",
    "\n",
    "mat_tup2cng_1D = {}\n",
    "for tup in mat_tup2cng.keys():\n",
    "    mainset = []\n",
    "    for fs in mat_tup2cng[tup].values():\n",
    "         mainset.extend(fs)\n",
    "    mainset = set(mainset)\n",
    "    mat_tup2cng_1D[tup] = len(mainset)\n",
    "    \n",
    "\n",
    "mat_tup2lem_1D = {}\n",
    "for tup in mat_tup2lem.keys():\n",
    "    mainset = []\n",
    "    for fs in mat_tup2lem[tup].values():\n",
    "         mainset.extend(fs)\n",
    "    mainset = set(mainset)\n",
    "    mat_tup2lem_1D[tup] = len(mainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SaNKa': ['192989.p'],\n",
       " 'Sukti': ['192989.p'],\n",
       " 'ca': ['192989.p'],\n",
       " 'drAkzA': ['192989.p'],\n",
       " 'maDu': ['192989.p'],\n",
       " 'maDuka': ['192989.p']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pickle.dump(mat_lem2cng_1D, open('../NewData/mat_lem2cng_1D.p', 'wb'))\n",
    "# pickle.dump(mat_tup2cng_1D, open('../NewData/mat_tup2cng_1D.p', 'wb'))\n",
    "# pickle.dump(mat_tup2lem_1D, open('../NewData/mat_tup2lem_1D.p', 'wb'))\n",
    "# len(mat_cng2lem)\n",
    "\n",
    "# display(mat_lem2cng.keys())\n",
    "display(mat_tup2lem['kataka_41'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the skt/dcs\n",
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66817\n",
      "276\n",
      "217326\n",
      "217326\n"
     ]
    }
   ],
   "source": [
    "print(len(mat_lem2cng))\n",
    "print(len(mat_cng2lem))\n",
    "print(len(mat_tup2cng))\n",
    "print(len(mat_tup2lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TheScores(q, v, c):\n",
    "#     print(q, v, c)\n",
    "    try:\n",
    "        if type(q) == str:\n",
    "            if type(v) == str:\n",
    "                p1 = ProbData.fullCo_oc_mat[q][v]/mat_lem2cng_1D[q]\n",
    "            elif type(v) == int:\n",
    "                p1 = len(mat_lem2cng[q][str(v)])/mat_lem2cng_1D[q]\n",
    "            elif type(v) == tuple:\n",
    "                z = v[0] + '_' + str(v[1])\n",
    "                p1 = len(mat_tup2lem[z][q])/mat_lem2cng_1D[q]\n",
    "        elif type(q) == int:\n",
    "            if type(v) == str:\n",
    "                p1 = len(mat_cng2lem[str(q)][v])/mat_cng2lem_1D[str(q)]\n",
    "            elif type(v) == tuple:\n",
    "                p1 = len(mat_tup2cng[v[0] + '_' + str(v[1])][str(q)])/mat_cng2lem_1D[str(q)]\n",
    "            elif type(v) == int:\n",
    "                ia = ProbData.cng2index_dict[str(q)]\n",
    "                ib = ProbData.cng2index_dict[str(v)]\n",
    "                p1 = ProbData.cng2cngFullMat[ia, ib]/mat_cng2lem_1D[str(q)]\n",
    "        elif type(q) == tuple:\n",
    "            z = q[0] + '_' + str(q[1])\n",
    "            if type(v) == str:\n",
    "                p1 = len(mat_tup2lem[z][v])/mat_tup2lem_1D[z]\n",
    "            elif type(v) == int:\n",
    "                p1 = len(mat_tup2cng[z][str(v)])/mat_tup2lem_1D[z]\n",
    "            \n",
    "        if type(c) == str:\n",
    "            if type(v) == str:\n",
    "                p2 = ProbData.fullCo_oc_mat[v][c]/mat_lem2cng_1D[v]\n",
    "            elif type(v) == int:\n",
    "                p2 = len(mat_cng2lem[str(v)][c])/mat_cng2lem_1D[v]\n",
    "            elif type(v) == tuple:\n",
    "                z = v[0] + '_' + str(v[1])\n",
    "                p2 = len(mat_tup2lem[z][c])/mat_tup2lem_1D[z]\n",
    "        elif type(c) == int:\n",
    "            if type(v) == str:\n",
    "                p2 = len(mat_lem2cng[v][str(c)])/mat_lem2cng_1D[v]\n",
    "            elif type(v) == int:\n",
    "                ia = ProbData.cng2index_dict[str(v)]\n",
    "                ib = ProbData.cng2index_dict[str(c)]\n",
    "                p2 = ProbData.cng2cngFullMat[ia, ib]/mat_cng2lem_1D[str(v)]\n",
    "            elif type(v) == tuple:\n",
    "                z = v[0] + '_' + str(v[1])\n",
    "                p2 = len(mat_tup2cng[z][str(c)])/mat_tup2lem_1D[z]\n",
    "        elif type(c) == tuple:\n",
    "            if type(v) == int:\n",
    "                p2 = len(mat_tup2cng[c[0] + '_' + str(c[1])][str(v)])/mat_cng2lem_1D[str(v)]\n",
    "            elif type(v) == str:\n",
    "                p2 = len(mat_tup2lem[c[0] + '_' + str(c[1])][v])/mat_lem2cng_1D[v]\n",
    "                \n",
    "        return p1*p2\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "## COPY IT BACK\n",
    "def ShortScore(q, c, code):\n",
    "    try:\n",
    "        if code == 'n-n':\n",
    "            if ProbData.fullCo_oc_mat[q][c] == 0:\n",
    "                return 0\n",
    "            p1 = ProbData.w2w_samecng_fullmat[q][c]/ProbData.fullCo_oc_mat[q][c]\n",
    "        else:\n",
    "            s1 = mat_selfLemCLASS_evidence[q]['verbs'] - mat_selfLemCLASS_evidence[q][code]\n",
    "            s2 = mat_selfLemCLASS_evidence[c][code]\n",
    "            if len(s1) == 0:\n",
    "                return 0\n",
    "            p1 = len(s1 & s2)/len(s1)\n",
    "    except KeyError:\n",
    "        p1 = 0\n",
    "    return p1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Get_QCs():\n",
    "    # Form NON-competitor dictionary - Query - Candidate Pairs\n",
    "    qc_pairs = {}\n",
    "    for ni in range(len(nodeList)):\n",
    "        qc_pairs[ni] = set(range(len(nodeList))) - set([ni])\n",
    "\n",
    "    for cid in chunkDict.keys():\n",
    "        # Neighbours\n",
    "        for pos1 in chunkDict[cid].keys():\n",
    "            for pos2 in chunkDict[cid].keys():\n",
    "                if pos1 <= pos2:\n",
    "                    nList1 = []\n",
    "                    for ti1 in chunkDict[cid][pos1]:\n",
    "                        for tup1 in tuplesMain[ti1]:\n",
    "                            nList1.append(tup1[0])\n",
    "                    nList2 = []\n",
    "                    for ti2 in chunkDict[cid][pos2]:\n",
    "                        for tup2 in tuplesMain[ti2]:\n",
    "                            nList2.append(tup2[0])\n",
    "                    nList1 = set(nList1)\n",
    "                    nList2 = set(nList2)\n",
    "                    for n1 in nList1:\n",
    "                        qc_pairs[n1] = qc_pairs[n1] - nList1\n",
    "\n",
    "                    for n2 in nList2:\n",
    "                        qc_pairs[n2] = qc_pairs[n2] - nList2\n",
    "\n",
    "                    if pos1 < pos2:\n",
    "                        for n1 in nList1:\n",
    "                            for n2 in nList2:\n",
    "                                if not CanCoExist_sandhi(pos1, pos2, nodeList[n1][1], nodeList[n2][1]):\n",
    "                                    qc_pairs[n1] = qc_pairs[n1] - set([n2])\n",
    "                                    qc_pairs[n2] = qc_pairs[n2] - set([n1])\n",
    "                                    \n",
    "    return qc_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIND THE SPECIEAL WORDS (REQUIRED IN CERTAIN PATHS) - TO BE USED/CHECKED AGAINST LATER\n",
    "\n",
    "# Pass before converting the nodeList\n",
    "def Splitter(nodeList):\n",
    "    nouns = set()\n",
    "    verbs = set()\n",
    "    # adverbs = set() # IGNORE ADVERB\n",
    "    gerund = set()\n",
    "    ppp = set()\n",
    "    ppa = set()\n",
    "    inf = set()\n",
    "    absol = set()\n",
    "\n",
    "\n",
    "    for n in nodeList:\n",
    "        if n[3] == -190:\n",
    "            ppp.add(n[0])\n",
    "        if n[3] == -200:\n",
    "            ppa.add(n[0])\n",
    "        if n[3] == -210: # Compare cng\n",
    "            gerund.add(n[0]) # Add id\n",
    "        if n[3] == -220:\n",
    "            inf.add(n[0])\n",
    "        if n[3] == -230:\n",
    "            absol.add(n[0])\n",
    "\n",
    "\n",
    "        # CONSIDERING ADV, GERUND, ABSOL ALSO IN VERB\n",
    "        if n[3] < 0:\n",
    "            verbs.add(n[0])\n",
    "        if n[3] > 3:\n",
    "            nouns.add(n[0])\n",
    "    return (nouns, verbs, gerund, ppp, ppa, inf, absol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nouns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9be8da31dc4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VB'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgerund\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PPP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mppp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nouns' is not defined"
     ]
    }
   ],
   "source": [
    "print('NN', nouns)\n",
    "print('VB',verbs)\n",
    "\n",
    "print('GE',gerund)\n",
    "print('PPP', ppp)\n",
    "print('PPA', ppa)\n",
    "print('INF',inf)\n",
    "print('ABS',absol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FIND EACH POSSIBLE QUERY-CANDIDATE PAIR AND CSV ALL THE AVAILABLE SCORES FOR THAT PAIR\n",
    "csvf = open('pcrw_01.csv', 'w')\n",
    "pcrw_csv = csv.writer(csvf)\n",
    "\n",
    "hList = ['f', 'ln_lemma', 'rn_lemma', 'ln_cng', 'rn_cng']\n",
    "for u in range(1,10):\n",
    "    if u < 4:\n",
    "        for v in [1,2,3]:\n",
    "            for w in [1,2,3]:\n",
    "                hList.append(u*100 + 10*v + w)\n",
    "    else:\n",
    "        hList.append(u*100)\n",
    "\n",
    "hList.append('flag')\n",
    "\n",
    "pcrw_csv.writerow(hList)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Loop through files\n",
    "for fn in list(loaded_SKT.keys())[0:10]:\n",
    "    skt  = loaded_SKT[fn]\n",
    "    dcs  = loaded_DCS[fn]\n",
    "\n",
    "    try:\n",
    "        (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain) = SentencePreprocess(skt)\n",
    "    except SentenceError:\n",
    "        continue    \n",
    "    \n",
    "    nodeList = [t for ts in tuplesMain for t in ts]\n",
    "    sol, solNoPvb = GetSolutions(dcs)\n",
    "    \n",
    "    (nouns, verbs, gerund, ppp, ppa, inf, absol) = Splitter(nodeList)\n",
    "    code_set = {'ger':(gerund, 500), 'absol':(absol, 600), 'ppp':(ppp, 700),\n",
    "                'inf':(inf, 800), 'ppa':(ppa, 900)}\n",
    "    \n",
    "    qc_pairs = Get_QCs()\n",
    "    \n",
    "    #Change nodelist id, lemma, cng, word\n",
    "    nodeList = [(t[0], t[2], t[3], (t[2], t[3])) for ts in tuplesMain for t in ts]\n",
    "\n",
    "    # def Get_lvl_score(nodeList, nouns, verbs, adverbs, path):\n",
    "    # 1 for lemma\n",
    "    # 2 for cng\n",
    "    # 3 for (lemma, cng)\n",
    "    for ri in range(len(nodeList)):\n",
    "        rn = nodeList[ri]\n",
    "\n",
    "        for li in qc_pairs[ri]:\n",
    "            scores = {}\n",
    "            for u in range(1,4):\n",
    "                for v in [1,2,3]:\n",
    "                    for w in [1,2,3]:\n",
    "                        scores[u*100 + 10*v + w] = 0\n",
    "            scores[400] = 0\n",
    "            scores[500] = 0\n",
    "            scores[600] = 0\n",
    "            scores[700] = 0\n",
    "            scores[800] = 0\n",
    "            scores[900] = 0\n",
    "            if ri > li: # Otherwise it will be measured twice - duplicate entries in csv\n",
    "                flag = 0\n",
    "                ln = nodeList[li]\n",
    "                if rn[1] in sol and ln[1] in sol:\n",
    "                    flag = 1\n",
    "                # FORM the paths here - OF LENGTH 3\n",
    "                for mi in qc_pairs[ri]:\n",
    "                    if mi != li  and mi in verbs:\n",
    "                        mn = nodeList[mi]\n",
    "                        for nt1 in [1, 2, 3]:\n",
    "                            for nt2 in [1, 2, 3]:\n",
    "                                for nt3 in [1, 2, 3]:\n",
    "                                    nts = [nt1, nt2, nt3]\n",
    "                                    if nts.count(3) > 1:\n",
    "                                        continue\n",
    "                                    scores[nt1*100 + nt2*10 + nt3] += TheScores(ln[nt1], mn[nt2], rn[nt3])\n",
    "\n",
    "                # FORM the paths of length 2\n",
    "                if ri in nouns and li in nouns:\n",
    "                    # 400 is code for noun-noun paths\n",
    "                    scores[400] += ShortScore(ln[1], rn[1], 'n-n')\n",
    "                \n",
    "                \n",
    "                \n",
    "                for key, pair in code_set.items():\n",
    "                    spSet = pair[0]\n",
    "                    code = pair[1]\n",
    "                    if li in verbs and ri in spSet and li not in spSet:\n",
    "                        scores[code] += ShortScore(ln[1], rn[1], key)\n",
    "\n",
    "                rowlist = [fn, ln[1], rn[1], ln[2], rn[2]]\n",
    "                for u in range(1,10):\n",
    "                    if u < 4:\n",
    "                        for v in [1,2,3]:\n",
    "                            for w in [1,2,3]:\n",
    "                                rowlist.append(scores[u*100 + 10*v + w])\n",
    "                    else:\n",
    "                        rowlist.append(scores[u*100])\n",
    "                rowlist.append(flag)\n",
    "                pcrw_csv.writerow(rowlist)\n",
    "\n",
    "\n",
    "\n",
    "    #             print(rn, ln, flag)\n",
    "\n",
    "            \n",
    "# Get_lvl_score(nodeList, nouns, verbs, adverbs, [0,0,0])\n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set([1,2,3])\n",
    "b = set([5,6,3])\n",
    "a & b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.union([4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.extend([4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_tup2lem_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
