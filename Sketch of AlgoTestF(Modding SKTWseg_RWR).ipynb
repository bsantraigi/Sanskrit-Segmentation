{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from AlgoTestFactory import *\n",
    "\n",
    "# altf1 = AlgoTestFactory([0, 10000], 15, savePath=\"Combined_4_test2\", storeAccuracies=True)\n",
    "\n",
    "# altf1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader Started[Prob]...\n",
      "Dataloader Finished[Prob]...\n"
     ]
    }
   ],
   "source": [
    "from SktWsegRWR_utf8 import *\n",
    "import pickle\n",
    "import ProbData\n",
    "from ProbModels import *\n",
    "import multiprocessing\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "import csv\n",
    "from utilities import *\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pb = ProbModels(fullCo_oc_mat = ProbData.fullCo_oc_mat, unigram_counts = ProbData.unigram_counts,\n",
    "               cng2cngFullMat = ProbData.cng2cngFullMat, cng2index_dict = ProbData.cng2index_dict,\n",
    "               w2w_samecng_fullmat=ProbData.w2w_samecng_fullmat, samecng_unigram_counts=ProbData.samecng_unigram_counts,\n",
    "               v2c_fullMat = ProbData.v2c_fullMat)\n",
    "\n",
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))\n",
    "\n",
    "algo = SktWsegRWR(\n",
    "            w2w_modelFunc = pb.get_w2w_mat, \n",
    "            t2t_modelFunc = pb.get_cng2cng_mat,\n",
    "            v2c_modelFunc = pb.get_v2c_ranking,\n",
    "            sameCng_modelFunc = pb.get_w2w_samecng_mat,\n",
    "            partition=[0.25, 0.25, 0.25, 0.1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# goodFileDict = {}\n",
    "# new_flist = os.listdir('../TextSegmentation/CompatSKT/')\n",
    "# for f in new_flist:\n",
    "#     goodFileDict[f] = '../TextSegmentation/CompatSKT/' + f\n",
    "# pickle.dump(goodFileDict, open('mergedGood_CompatSKT.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100078"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pf1 = pd.read_csv('extras/pvbnhi@pf.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "# df_pf2 = pd.read_csv('extras/pvbnhi@skt.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "# df_pf3 = pd.read_csv('extras/pvbnhi@upd.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "\n",
    "# df_pf1['miss'] = df_pf1[df_pf1['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "# df_pf1 = df_pf1[df_pf1['miss'] == 0]\n",
    "# df_pf1['folder'] = '../TextSegmentation/Pickle_Files/'\n",
    "\n",
    "# df_pf2['miss'] = df_pf2[df_pf2['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "# df_pf2 = df_pf2[df_pf2['miss'] == 0]\n",
    "# df_pf2['folder'] = '../TextSegmentation/corrected_10to20/'\n",
    "\n",
    "# df_pf3['miss'] = df_pf3[df_pf3['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "# df_pf3 = df_pf3[df_pf3['miss'] == 0]\n",
    "# df_pf3['folder'] = '../TextSegmentation/Updated Pickles/'\n",
    "\n",
    "# frames = [df_pf1, df_pf2, df_pf3]\n",
    "# df_pf = pd.concat(frames)\n",
    "# %reset_selective df_pf1\n",
    "# %reset_selective df_pf2\n",
    "# %reset_selective df_pf3\n",
    "# goodFileDict = {}\n",
    "# for index, row in df_pf.iterrows():\n",
    "#     goodFileDict['%d.p' % row['file']] = '%s%d.p' % (row['folder'], row['file'])\n",
    "len(goodFileDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49441"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algo.partition = np.array([0.33,0.33,0.33,0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31031.p2 \t 66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "fs = ['31031.p2']\n",
    "np.set_printoptions(precision=4)\n",
    "# for f in list(goodFileDict.keys())[1:20]:\n",
    "# for f in list(goodFileDict.keys())[1:100]:\n",
    "for f in fs:\n",
    "#     print('=='*20)\n",
    "    sentenceObj, dcsObj = loadSentence_nopre(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result = algo.predict(sentenceObj, dcsObj, verbose = False, supervised=True, eta=0.1)\n",
    "        if(result != None):\n",
    "            solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "            ac = Accuracy(result, dcsObj)\n",
    "            print(f, \"\\t\", ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadSentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-8e34165bc39f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'30240.p'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# print(goodFileDict[f])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../TextSegmentation/Updated Pickles/30240.p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mSeeSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mSeeDCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loadSentence' is not defined"
     ]
    }
   ],
   "source": [
    "f = '30240.p'\n",
    "# print(goodFileDict[f])\n",
    "s, d = loadSentence(f, '../TextSegmentation/Updated Pickles/30240.p')\n",
    "SeeSentence(s)\n",
    "SeeDCS(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43960.p2 \t 20.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['prajApati'], ['BfSASva'], ['putra'], ['satya', 'parAkrama']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['putra',\n",
       " 'as',\n",
       " 'idam',\n",
       " 'para',\n",
       " 'a',\n",
       " 'a',\n",
       " 'krama',\n",
       " 'ASu',\n",
       " 'prajA',\n",
       " 'pat',\n",
       " 'a',\n",
       " 'BfSa']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Query: [33]\n",
      "------------------------------\n",
      "Step: 0\n",
      "------------------------------\n",
      "Winner: (37, 'sati', 'as', -10)\n",
      "Removed: [(35, 'sati', 'sat', 49), (36, 'sati', 'sat', 51), (34, 'satya', 'satya', 3), (38, 'satI', 'sat', 76), (39, 'satI', 'sat', 30), (40, 'satI', 'sat', 36), (41, 'satI', 'sat', 3)]\n",
      "Query: [33, 37]\n",
      "[[ 0.0161  0.0161  0.0232  0.0023  0.0002  0.0023  0.0023  0.0023  0.0022\n",
      "   0.0022  0.0013  0.0012  0.0003  0.0238  0.      0.      0.0238  0.0006\n",
      "   0.0238  0.0027  0.0027  0.0027  0.0026  0.0028  0.0028  0.0028  0.0028\n",
      "   0.      0.      0.      0.0701  0.0701  0.0039  0.4235  0.0071  0.      0.\n",
      "   0.0057  0.      0.      0.      0.      0.0413  0.0413  0.0002  0.0452\n",
      "   0.1066  0.      0.0001  0.0001  0.0002  0.0183]]\n",
      "------------------------------\n",
      "Step: 1\n",
      "------------------------------\n",
      "Winner: (30, 'asya', 'idam', 149)\n",
      "Removed: [(31, 'asya', 'idam', 151), (13, 'aSvasya', 'aSva', 149), (14, 'ASvasya', 'ASva', 149), (15, 'ASvasya', 'ASva', 151), (16, 'aSvasya', 'aSva', 151), (17, 'ASvasya', 'Svas', -230), (18, 'aSvasya', 'aSva', -32), (27, 'asya', 'asUya', 49), (28, 'asya', 'asUya', 51), (29, 'asya', 'asUya', -210), (32, 'asya', 'as', -32)]\n",
      "Query: [33, 37, 30]\n",
      "[[ 0.0115  0.0115  0.0127  0.0237  0.0002  0.0237  0.0237  0.0236  0.0236\n",
      "   0.0237  0.0011  0.001   0.0002  0.0143  0.      0.      0.0143  0.0022\n",
      "   0.0143  0.0022  0.0023  0.0023  0.0022  0.0024  0.0024  0.0024  0.0024\n",
      "   0.      0.      0.      0.0542  0.0542  0.0337  0.2173  0.      0.      0.\n",
      "   0.2343  0.      0.      0.      0.      0.0267  0.0267  0.0002  0.0244\n",
      "   0.0701  0.      0.      0.      0.0002  0.0142]]\n",
      "------------------------------\n",
      "Step: 2\n",
      "------------------------------\n",
      "Winner: (46, 'para', 'para', 3)\n",
      "Removed: [(42, 'apara', 'apara', 3), (43, 'aparA', 'apara', 30), (45, 'parAkramAn', 'parAkrama', 79), (47, 'pa', 'pa', 3), (48, 'rA', 'rA', 89), (49, 'rA', 'rA', 30)]\n",
      "Query: [33, 37, 30, 46, 44, 50, 51]\n",
      "[[ 0.0219  0.0219  0.0132  0.017   0.0002  0.017   0.017   0.0169  0.017\n",
      "   0.0171  0.0013  0.0013  0.0002  0.      0.      0.      0.      0.      0.\n",
      "   0.0031  0.0032  0.0032  0.0031  0.0032  0.0032  0.0032  0.0032  0.      0.\n",
      "   0.      0.2308  0.      0.      0.1555  0.      0.      0.      0.1771\n",
      "   0.      0.      0.      0.      0.0406  0.0406  0.0002  0.0255  0.114\n",
      "   0.      0.0001  0.0001  0.0002  0.0278]]\n",
      "------------------------------\n",
      "Step: 3\n",
      "------------------------------\n",
      "Winner: (26, 'ASu', 'ASu', 2)\n",
      "Removed: [(12, 'BfSASva', 'BfSASva', 149), (19, 'ASu', 'ASu', 3), (20, 'ASu', 'ASu', 31), (21, 'ASu', 'ASu', 71), (22, 'ASU', 'ASu', 35), (23, 'ASU', 'ASu', 74), (24, 'ASU', 'ASu', 75), (25, 'ASU', 'ASu', 34)]\n",
      "Query: [33, 37, 30, 46, 44, 50, 51, 26]\n",
      "[[ 0.0297  0.0297  0.015   0.0108  0.0004  0.0108  0.0108  0.0108  0.0108\n",
      "   0.0108  0.0024  0.0023  0.0003  0.      0.      0.      0.      0.      0.\n",
      "   0.0063  0.0063  0.0063  0.0061  0.0063  0.0062  0.0062  0.0063  0.      0.\n",
      "   0.      0.2448  0.      0.      0.0837  0.      0.      0.      0.0895\n",
      "   0.      0.      0.      0.      0.      0.      0.0575  0.      0.1751\n",
      "   0.      0.      0.      0.0575  0.0972]]\n",
      "------------------------------\n",
      "Step: 4\n",
      "------------------------------\n",
      "Winner: (0, 'prajA', 'prajA', 3)\n",
      "Removed: [(1, 'prajA', 'prajA', 30), (2, 'prajApati', 'prajApati', 149)]\n",
      "Query: [33, 37, 30, 46, 44, 50, 51, 26, 0]\n",
      "[[ 0.0323  0.0323  0.0158  0.0107  0.0005  0.0107  0.0107  0.0106  0.0107\n",
      "   0.0107  0.0026  0.0026  0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.0566  0.      0.\n",
      "   0.      0.257   0.      0.      0.0795  0.      0.      0.      0.084\n",
      "   0.      0.      0.      0.      0.      0.      0.0504  0.      0.1783\n",
      "   0.      0.      0.      0.0504  0.0939]]\n",
      "------------------------------\n",
      "Step: 5\n",
      "------------------------------\n",
      "Winner: (6, 'pateḥ', 'pat', 149)\n",
      "Removed: [(5, 'pateḥ', 'pat', 129), (3, 'Apateḥ', 'pat', -22), (7, 'pateḥ', 'pat', 130), (8, 'pateḥ', 'pat', 150), (9, 'pateḥ', 'pat', -22)]\n",
      "Query: [33, 37, 30, 46, 44, 50, 51, 26, 0, 6, 4]\n",
      "[[ 0.0841  0.      0.      0.0103  0.0005  0.0103  0.0103  0.0103  0.0103\n",
      "   0.0103  0.0027  0.0027  0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.0515  0.      0.\n",
      "   0.      0.2673  0.      0.      0.0702  0.      0.      0.      0.0779\n",
      "   0.      0.      0.      0.      0.      0.      0.0449  0.      0.1948\n",
      "   0.      0.      0.      0.0449  0.0969]]\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "algo.partition = np.array([0.8,0.33,0.33,0.01])\n",
    "fs = ['43960.p2']\n",
    "def debugSentences(fs):\n",
    "    np.set_printoptions(precision=4)\n",
    "    # for f in list(goodFileDict.keys())[1:20]:\n",
    "    # for f in list(goodFileDict.keys())[1:100]:\n",
    "    for f in fs:\n",
    "    #     print('=='*20)\n",
    "        sentenceObj, dcsObj = loadSentence_nopre(f, goodFileDict[f])\n",
    "        solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "        solution_no_pvb = [removePrefix(l) for l in solution]\n",
    "#         print('='*40)\n",
    "#         print(dcsObj.lemmas)\n",
    "#         print(solution)\n",
    "\n",
    "#         SeeSentence(sentenceObj)\n",
    "        if(sentenceObj != None):\n",
    "            result, detail = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "            if(result != None):\n",
    "                solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "                ac = Accuracy(result, dcsObj)\n",
    "                print(f, \"\\t\", ac)\n",
    "#                 display(detail['sentence'])\n",
    "                display(detail['DCSLemmas'])\n",
    "                \n",
    "                display(result)\n",
    "#                 print(detail['nodeList'])\n",
    "                print('Initial Query:', detail['initialQuery'])\n",
    "                for step in range(detail['steps']):\n",
    "                    print('-'*30)\n",
    "                    print('Step:', step)\n",
    "                    print('-'*30)\n",
    "                    print('Winner:', detail[str(step)]['winner'])\n",
    "                    print('Removed:', detail[str(step)]['removed'])\n",
    "                    print('Query:', detail[str(step)]['updated_query'])\n",
    "                    print(detail[str(step)]['w2w_samecng_score'])\n",
    "        print('='*30)\n",
    "\n",
    "debugSentences(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty name in file 240430\n",
      "Empty name in file 333881\n",
      "Empty name in file 7302\n",
      "Empty name in file 32452\n",
      "Empty name in file 389091\n",
      "Empty name in file 431033\n",
      "Empty name in file 310144\n",
      "Empty name in file 131042\n",
      "Empty name in file 32130\n",
      "Empty name in file 19229\n",
      "Empty name in file 156107\n",
      "Empty name in file 190965\n"
     ]
    }
   ],
   "source": [
    "defaultPartition = [0.33,0.33,0.33,0]\n",
    "\n",
    "# algo.partition = np.random.random(size=(4,))\n",
    "algo.partition = np.array(defaultPartition)\n",
    "algo.partition /= np.sum(algo.partition)\n",
    "\n",
    "with open('.temp/partition_datas_03_noreset_but_normalize_0.1eta.csv', 'w') as wcsv_fh:\n",
    "    wcsv = csv.writer(wcsv_fh)\n",
    "    for f in loaded_SKT.keys():\n",
    "    # for f in fullfs:\n",
    "        sentenceObj = loaded_SKT[f]\n",
    "        dcsObj = loaded_DCS[f]\n",
    "        if(sentenceObj != None):\n",
    "#             if any(algo.partition[:-1:] < 0.05) or any(algo.partition[:-1:] > 0.95):\n",
    "#                 wcsv.writerow(algo.partition)\n",
    "# #                 algo.partition = np.random.random(size=(4,))\n",
    "#                 algo.partition = np.array(defaultPartition)\n",
    "#                 algo.partition /= np.sum(algo.partition)\n",
    "            result = algo.predict(sentenceObj, dcsObj, supervised=True, eta = 0.1, weightCollectorCSV = wcsv)\n",
    "            if(result != None):\n",
    "                ac = Accuracy(result, dcsObj)\n",
    "                solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "#                 print(result)\n",
    "#                 print('vs')\n",
    "#                 print(solution)\n",
    "\n",
    "#                 print(f, \"\\t\", ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32  0.33  0.32  0.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.33,  0.34,  0.33,  0.  ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(algo.partition)\n",
    "algo.partition/np.sum(algo.partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fullfs = ['105587.p','293473.p','379245.p']\n",
    "fullfs = ['302864.p', '292204.p', '161425.p', '128533.p', '265112.p', '16349.p', '320178.p', '334655.p', '331468.p', '365975.p', '434538.p', '148267.p', '340997.p', '204751.p', '289589.p', '423330.p', '276728.p', '105056.p', '325962.p', '383515.p', '60055.p', '228316.p', '192549.p', '169215.p', '393452.p', '433996.p', '440103.p', '68686.p', '157189.p', '440567.p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadSentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4bbaad956bd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#=================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mGetSomeJSON\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-4bbaad956bd0>\u001b[0m in \u001b[0;36mGetSomeJSON\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#     print(len(fullfs))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfullfs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msentenceObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdcsObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoodFileDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceObj\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunDetails\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdcsObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loadSentence' is not defined"
     ]
    }
   ],
   "source": [
    "fullfs = ['302864.p', '292204.p', '161425.p', '128533.p', '265112.p', '16349.p', '320178.p', '334655.p', '331468.p', '365975.p', '434538.p', '148267.p', '340997.p', '204751.p', '289589.p', '423330.p', '276728.p', '105056.p', '325962.p', '383515.p', '60055.p', '228316.p', '192549.p', '169215.p', '393452.p', '433996.p', '440103.p', '68686.p', '157189.p', '440567.p']\n",
    "def GetSomeJSON():\n",
    "    lowScore = []\n",
    "    midScore = []\n",
    "    highScore = []\n",
    "    np.set_printoptions(precision=4)\n",
    "#     for i in range(1, 400):\n",
    "#         f = list(goodFileDict.keys())[i]\n",
    "#     print(len(fullfs))\n",
    "    for f in fullfs:\n",
    "        sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "        if(sentenceObj != None):\n",
    "            result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "\n",
    "            if(result != None):\n",
    "                runDetails['file'] = f\n",
    "                ac = runDetails['accuracy']\n",
    "#                 print(f, ac)\n",
    "\n",
    "                # CODE FOR SELECTING FILES \n",
    "#                 if(i%100 == 0):\n",
    "#                     print('Chekpoint:', i)\n",
    "                if(ac <= 40):\n",
    "                    lowScore.append(runDetails)\n",
    "                elif (ac <= 60):\n",
    "                    if(ac >= 45):\n",
    "                        midScore.append(runDetails)\n",
    "                elif (ac <= 100):\n",
    "                    if(ac >= 80):\n",
    "                        highScore.append(runDetails)\n",
    "\n",
    "    print('Lowscore files found:', len(lowScore))\n",
    "    print('Midscore files found:', len(midScore))\n",
    "    print('Highscore files found:', len(highScore))\n",
    "\n",
    "    lowScore = lowScore[0:10]\n",
    "    midScore = midScore[0:10]\n",
    "    highScore = highScore[0:10]\n",
    "    pickle.dump(lowScore, open('lowScore.p', 'wb'))\n",
    "    pickle.dump(midScore, open('midScore.p', 'wb'))\n",
    "    pickle.dump(highScore, open('highScore.p', 'wb'))\n",
    "\n",
    "#=================================================================\n",
    "GetSomeJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chekpoint: 100\n",
      "Chekpoint: 200\n",
      "Chekpoint: 400\n",
      "Chekpoint: 500\n",
      "Chekpoint: 600\n",
      "Chekpoint: 700\n",
      "Chekpoint: 800\n",
      "Chekpoint: 900\n",
      "Chekpoint: 1000\n",
      "Chekpoint: 1100\n",
      "Chekpoint: 1300\n",
      "Chekpoint: 1400\n",
      "Empty name in file 240430\n",
      "Chekpoint: 1500\n",
      "Chekpoint: 1600\n",
      "Chekpoint: 1700\n",
      "Chekpoint: 1800\n",
      "Chekpoint: 1900\n",
      "Chekpoint: 2000\n",
      "Chekpoint: 2100\n",
      "Chekpoint: 2200\n",
      "Chekpoint: 2300\n",
      "Chekpoint: 2400\n",
      "Chekpoint: 2500\n",
      "Empty name in file 333881\n",
      "Chekpoint: 2600\n",
      "Chekpoint: 2700\n",
      "Chekpoint: 2800\n",
      "Empty name in file 7302\n",
      "Chekpoint: 2900\n",
      "Chekpoint: 3000\n",
      "Chekpoint: 3100\n",
      "Chekpoint: 3200\n",
      "Chekpoint: 3300\n",
      "Chekpoint: 3400\n",
      "Chekpoint: 3500\n",
      "Chekpoint: 3700\n",
      "Chekpoint: 3900\n",
      "Chekpoint: 4000\n",
      "Chekpoint: 4100\n",
      "Chekpoint: 4200\n",
      "Chekpoint: 4400\n",
      "Chekpoint: 4500\n",
      "Chekpoint: 4600\n",
      "Chekpoint: 4700\n",
      "Chekpoint: 4800\n",
      "Chekpoint: 4900\n",
      "Chekpoint: 5000\n",
      "Chekpoint: 5100\n",
      "Chekpoint: 5200\n",
      "Chekpoint: 5300\n",
      "Empty name in file 32452\n",
      "Chekpoint: 5500\n",
      "Chekpoint: 5600\n",
      "Chekpoint: 5800\n",
      "Empty name in file 389091\n",
      "Chekpoint: 5900\n",
      "Chekpoint: 6000\n",
      "Chekpoint: 6100\n",
      "Empty name in file 431033\n",
      "Chekpoint: 6200\n",
      "Chekpoint: 6300\n",
      "Chekpoint: 6400\n",
      "Chekpoint: 6500\n",
      "Empty name in file 310144\n",
      "Chekpoint: 6600\n",
      "Chekpoint: 6700\n",
      "Empty name in file 131042\n",
      "Chekpoint: 6800\n",
      "Chekpoint: 6900\n",
      "Chekpoint: 7000\n",
      "Chekpoint: 7100\n",
      "Chekpoint: 7200\n",
      "Chekpoint: 7300\n",
      "Chekpoint: 7400\n",
      "Chekpoint: 7600\n",
      "Chekpoint: 7700\n",
      "Chekpoint: 7800\n",
      "Chekpoint: 7900\n",
      "Chekpoint: 8000\n",
      "Chekpoint: 8100\n",
      "Chekpoint: 8200\n",
      "Chekpoint: 8300\n",
      "Chekpoint: 8400\n",
      "Chekpoint: 8500\n",
      "Empty name in file 32130\n",
      "Chekpoint: 8600\n",
      "Chekpoint: 8700\n",
      "Chekpoint: 8800\n",
      "Chekpoint: 8900\n",
      "Chekpoint: 9000\n",
      "Empty name in file 19229\n",
      "Chekpoint: 9300\n",
      "Empty name in file 156107\n",
      "Chekpoint: 9400\n",
      "Chekpoint: 9500\n",
      "Empty name in file 190965\n",
      "Chekpoint: 9700\n",
      "Chekpoint: 9800\n",
      "Chekpoint: 9900\n",
      "Chekpoint: 10000\n"
     ]
    }
   ],
   "source": [
    "def MacroAccuracy():\n",
    "    allAcs = []\n",
    "    np.set_printoptions(precision=4)\n",
    "    algo.partition = np.array([0.35, 0.1, 0.55, 0.0])\n",
    "    i = 0\n",
    "    for f in loaded_SKT.keys():\n",
    "        i += 1\n",
    "    # for f in fullfs:\n",
    "        sentenceObj = loaded_SKT[f]\n",
    "        dcsObj = loaded_DCS[f]\n",
    "        if(sentenceObj != None):\n",
    "            result = algo.predict(sentenceObj, dcsObj, verbose = False)\n",
    "            \n",
    "            if(result != None):\n",
    "#                 print(result)\n",
    "#                 print(dcsObj.lemmas)\n",
    "                ac = Accuracy(result, dcsObj)\n",
    "                allAcs.append(ac)\n",
    "#                 print(f, ac)\n",
    "\n",
    "                # CODE FOR SELECTING FILES \n",
    "                if(i%100 == 0):\n",
    "                    print('Chekpoint:', i)\n",
    "    allAcs = np.array(allAcs)\n",
    "    return allAcs\n",
    "\n",
    "\n",
    "#=================================================================\n",
    "allAcs = MacroAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition: [ 0.35  0.1   0.55  0.  ]\n",
      "8756\n",
      "74.0184180574\n",
      "Min: 0.000000, Max: 100.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEZCAYAAAC99aPhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f569f73d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Accuracy %: 16.2174508908\n"
     ]
    }
   ],
   "source": [
    "print('Partition:', algo.partition)\n",
    "print(len(allAcs))\n",
    "print(np.mean(allAcs))\n",
    "print('Min: %f, Max: %f' % (np.min(allAcs), np.max(allAcs)))\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(allAcs, 4, normed=False)\n",
    "plt.xlabel('MicroAccuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of MicroAccuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('Macro Accuracy %:', 100*np.sum(allAcs >= 95)/allAcs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SeeSomeJSON(region = 'mid', files = ['lowScore.p', 'midScore.p', 'highScore.p']):\n",
    "    np.set_printoptions(precision=3)\n",
    "    if region == 'low':\n",
    "        p = pickleFixLoad(files[0])\n",
    "    elif region == 'mid':\n",
    "        p = pickleFixLoad(files[1])\n",
    "    else:\n",
    "        p = pickleFixLoad(files[2])\n",
    "#     for pf in p:\n",
    "#         print(pf['file'])\n",
    "    n_steps = p[0]['steps']\n",
    "    for s in range(n_steps):\n",
    "        \n",
    "        print('Step:', s)\n",
    "        print('='*20)\n",
    "        \n",
    "        for key, val in p[0][str(s)].items():\n",
    "            print(key, \":\")\n",
    "            print(val)\n",
    "            print('-'*20)\n",
    "        \n",
    "SeeSomeJSON('low')\n",
    "print('='*20)\n",
    "SeeSomeJSON('mid')\n",
    "print('='*20)\n",
    "SeeSomeJSON('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "35 weird predictions encountered.\n"
     ]
    }
   ],
   "source": [
    "def ArePredictionsOK():\n",
    "    with open('.temp/weird.csv', 'w') as fh:\n",
    "        wcsv = csv.writer(fh)\n",
    "        wcsv.writerow(['filename', 'sentence', 'solution', 'prediction'])\n",
    "        np.set_printoptions(precision=4)\n",
    "        badCount = 0\n",
    "        for i in range(1, 600):\n",
    "            f = list(goodFileDict.keys())[i]\n",
    "\n",
    "            sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "            if(sentenceObj != None):\n",
    "                result = algo.predict(sentenceObj, dcsObj, verbose = False)\n",
    "                if(result != None):            \n",
    "                    solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "                    ac = Accuracy(result, dcsObj)\n",
    "                    if ac >= 100 and len(result) != len(solution):\n",
    "                        badCount += 1\n",
    "                        if(badCount%10 == 0):\n",
    "                            print(\"=\"*40)\n",
    "                        sline = [f, sentenceObj.sentence, solution, result]\n",
    "                        wcsv.writerow(sline)\n",
    "        print(badCount, 'weird predictions encountered.')\n",
    "        \n",
    "ArePredictionsOK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKT ANALYZE\n",
      "---------------\n",
      "paYca ratnAni muKyAni coparatnacatuzwayam   \n",
      "Analyzing: paYca\n",
      "-------------------------\n",
      "Analyzing: ratnAni\n",
      "0 : ratnAni [0, 'ratna', 81] [1, 'ratna', 41] \n",
      "0 : ratnAni [2, 'ratna', 61] \n",
      "-------------------------\n",
      "Analyzing: muKyAni\n",
      "0 : muKyAni [3, 'muKya', 81] [4, 'muKya', 41] \n",
      "0 : muKyAni [5, 'muKya', 61] \n",
      "-------------------------\n",
      "Analyzing: coparatnacatuzwayam\n",
      "0 : ca [6, 'ca', 2] \n",
      "9 : catuzwayam [7, 'catuzwaya', 31] [8, 'catuzwaya', 69] [9, 'catuzwaya', 71] \n",
      "4 : ratna [10, 'ratna', 3] \n",
      "1 : uparatna [11, 'uparatna', 3] \n",
      "1 : Upa [12, 'vap', -158] \n",
      "-------------------------\n",
      "Analyzing  paYca\n",
      "0 :  paYca [] []\n",
      "Analyzing  ratnAni\n",
      "0 :  ratnAni ['ratna'] [{'noun': ['acc. pl. n.', 'nom. pl. n.']}]\n",
      "0 :  ratnAni ['ratna'] [{'noun': ['voc. pl. n.']}]\n",
      "Analyzing  muKyAni\n",
      "0 :  muKyAni ['muKya'] [{'noun': ['acc. pl. n.', 'nom. pl. n.']}]\n",
      "0 :  muKyAni ['muKya'] [{'noun': ['voc. pl. n.']}]\n",
      "Analyzing  coparatnacatuzwayam\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "9 :  catuzwayam ['catuzwaya'] [{'noun': ['acc. sg. m.', 'acc. sg. n.', 'nom. sg. n.']}]\n",
      "4 :  ratna ['ratna'] [{'compound': ['iic.']}]\n",
      "1 :  uparatna ['uparatna'] [{'compound': ['iic.']}]\n",
      "1 :  Upa ['vap', 'vap'] [{'verb': ['pft. ac. pl. 2']}, {'verb': ['pft. ac. pl. 2']}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceObj, dcsObj = loadSentence('1.p', '../TextSegmentation/corrected_10to20/1.p')\n",
    "SeeSentence(sentenceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCS ANALYZE\n",
      "---------------\n",
      "upacAreRa SIlena rUpayOvanasaMpadA   \n",
      "[['upacāra'], ['śīla'], ['rūpa', 'yauvana', 'sampad']]\n",
      "Lemmas: ['upacAra', 'SIla', 'rUpa', 'yOvana', 'sampad']\n",
      "[['89'], ['91'], ['3', '3', '90']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SeeDCS(dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CSVforSupervised():\n",
    "    np.set_printoptions(precision=4)\n",
    "    # Clean the csv files\n",
    "    metPerfFH = open('.temp/metPerfFH.csv', 'w')\n",
    "    metPerfFH_bin = open('.temp/metPerfFH_bin.csv', 'w')\n",
    "\n",
    "    metPerfFH.write('w2w,t2t,w2w_samecng,InDCS\\n')\n",
    "    metPerfFH_bin.write('w2w,t2t,w2w_samecng,InDCS\\n')\n",
    "\n",
    "    metPerfFH.close()\n",
    "    metPerfFH_bin.close()\n",
    "    ######################\n",
    "    gfl = list(goodFileDict.keys())\n",
    "    for i in range(0, 11000):\n",
    "        f = gfl[i]\n",
    "        if(i%250 == 0):\n",
    "            print('Checkpoint:', i)\n",
    "        sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "        \n",
    "        if(sentenceObj != None):\n",
    "            result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6)\n",
    "jsonBatch = []\n",
    "for f_ in lowScore[0:10]:\n",
    "    f = f_[0]\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "        runDetails['file'] = f\n",
    "        jsonBatch.append(runDetails)\n",
    "pickle.dump(jsonBatch, open('.temp/lowScore.p','wb'))\n",
    "print('33 % Complete')\n",
    "#################################################################\n",
    "jsonBatch = []\n",
    "for f_ in midScore[0:10]:\n",
    "    f = f_[0]\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "        runDetails['file'] = f\n",
    "        jsonBatch.append(runDetails)\n",
    "pickle.dump(jsonBatch, open('.temp/midScore.p','wb'))\n",
    "print('67 % Complete')\n",
    "#################################################################\n",
    "jsonBatch = []\n",
    "for f_ in highScore[0:10]:\n",
    "    f = f_[0]\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "        runDetails['file'] = f\n",
    "        jsonBatch.append(runDetails)\n",
    "print(pprint.pprint(jsonBatch))\n",
    "pickle.dump(jsonBatch, open('.temp/highScore.p','wb'))\n",
    "print('100 % Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_SKTs = pickle.load(open('../Simult_baselineSKT.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't form query node in 1076 files\n"
     ]
    }
   ],
   "source": [
    "def DownloadBaselineOutput():\n",
    "    pb = ProbModels(fullCo_oc_mat = ProbData.fullCo_oc_mat, unigram_counts = ProbData.unigram_counts,\n",
    "               cng2cngFullMat = ProbData.cng2cngFullMat, cng2index_dict = ProbData.cng2index_dict,\n",
    "               w2w_samecng_fullmat=ProbData.w2w_samecng_fullmat, samecng_unigram_counts=ProbData.samecng_unigram_counts,\n",
    "               v2c_fullMat = ProbData.v2c_fullMat)\n",
    "\n",
    "    algo = SktWsegRWR(\n",
    "                w2w_modelFunc = pb.get_w2w_mat, \n",
    "                t2t_modelFunc = pb.get_cng2cng_mat,\n",
    "                v2c_modelFunc = pb.get_v2c_ranking,\n",
    "                sameCng_modelFunc = pb.get_w2w_samecng_mat,\n",
    "                partition=[0.25, 0.25, 0.25, 0.1]\n",
    "            )\n",
    "    \n",
    "    counter = 0\n",
    "    no_query = 0\n",
    "    with open('baseline_outputs_post.csv', 'w') as fh:\n",
    "        bcsv = csv.writer(fh)\n",
    "        bcsv.writerow(['filename', 'lemma', 'name', 'cng', 'pos', 'source'])\n",
    "        for f in baseline_SKTs.keys():\n",
    "            counter += 1\n",
    "#             if counter % 200 == 0:\n",
    "#                 print('chekpoint', counter)\n",
    "#                 if counter > 20:\n",
    "#                     break\n",
    "\n",
    "            skt = baseline_SKTs[f]\n",
    "            if skt != None:\n",
    "                result, runDetails = algo.predict(skt, None, verbose = True)\n",
    "                if result == None:\n",
    "                    no_query += 1\n",
    "#                     SeeSentence(skt)\n",
    "                    continue\n",
    "                (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain) = SentencePreprocess(skt)\n",
    "    #             print(runDetails.keys())\n",
    "    #             SeeSentence(skt)\n",
    "    #             display(runDetails['nodeList'])\n",
    "    #             print('STEPS:', int(runDetails['steps']))\n",
    "    #             print(eval(runDetails['initialQuery']))\n",
    "                for qi in eval(runDetails['initialQuery']):\n",
    "                    cid, pos, tid = revMap2Chunk[int(qi)]\n",
    "    #                 print([f, lemmaList[qi], wordList[qi], cngList[qi], pos, 'initial_query'])\n",
    "                    bcsv.writerow([f, lemmaList[qi], wordList[qi], cngList[qi], pos, 'initial_query'])\n",
    "                for step in range(int(runDetails['steps'])):\n",
    "                    wt = runDetails[str(step)]['winner']\n",
    "                    cid, pos, tid = revMap2Chunk[wt[0]]\n",
    "    #                 print([f, wt[2], wt[1], wt[3], pos, 'winner'])\n",
    "                    bcsv.writerow([f, wt[2], wt[1], wt[3], pos, 'winner'])\n",
    "            else:\n",
    "                no_query += 1\n",
    "\n",
    "    print('Can\\'t form query node in', no_query, 'files')\n",
    "DownloadBaselineOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chekpoint 400\n",
      "chekpoint 800\n",
      "chekpoint 1200\n",
      "Empty name in file 240430\n",
      "chekpoint 1600\n",
      "chekpoint 2000\n",
      "chekpoint 2400\n",
      "Empty name in file 333881\n",
      "chekpoint 2800\n",
      "Empty name in file 7302\n",
      "chekpoint 3200\n",
      "chekpoint 3600\n",
      "chekpoint 4000\n",
      "chekpoint 4400\n",
      "chekpoint 4800\n",
      "chekpoint 5200\n",
      "Empty name in file 32452\n",
      "chekpoint 5600\n",
      "Empty name in file 389091\n",
      "chekpoint 6000\n",
      "Empty name in file 431033\n",
      "chekpoint 6400\n",
      "Empty name in file 310144\n",
      "Empty name in file 131042\n",
      "chekpoint 6800\n",
      "chekpoint 7200\n",
      "chekpoint 7600\n",
      "chekpoint 8000\n",
      "chekpoint 8400\n",
      "Empty name in file 32130\n",
      "chekpoint 8800\n",
      "Empty name in file 19229\n",
      "chekpoint 9200\n",
      "Empty name in file 156107\n",
      "chekpoint 9600\n",
      "Empty name in file 190965\n",
      "chekpoint 10000\n",
      "Can't form query node in 1244 files\n"
     ]
    }
   ],
   "source": [
    "def GetLogReg_TrainData():\n",
    "    pb = ProbModels(fullCo_oc_mat = ProbData.fullCo_oc_mat, unigram_counts = ProbData.unigram_counts,\n",
    "               cng2cngFullMat = ProbData.cng2cngFullMat, cng2index_dict = ProbData.cng2index_dict,\n",
    "               w2w_samecng_fullmat=ProbData.w2w_samecng_fullmat, samecng_unigram_counts=ProbData.samecng_unigram_counts,\n",
    "               v2c_fullMat = ProbData.v2c_fullMat)\n",
    "\n",
    "    algo = SktWsegRWR(\n",
    "                w2w_modelFunc = pb.get_w2w_mat, \n",
    "                t2t_modelFunc = pb.get_cng2cng_mat,\n",
    "                v2c_modelFunc = pb.get_v2c_ranking,\n",
    "                sameCng_modelFunc = pb.get_w2w_samecng_mat,\n",
    "                partition=[0.35, 0.1, 0.55, 0.0]\n",
    "            )\n",
    "    \n",
    "    counter = 0\n",
    "    no_query = 0\n",
    "    with open('logReg_train_full.csv', 'w') as fh:\n",
    "        bcsv = csv.writer(fh)\n",
    "        bcsv.writerow(['w2w', 't2t', 'scng', 'DSCLemma'])\n",
    "        for f in loaded_SKT.keys():\n",
    "            counter += 1\n",
    "            if counter % 400 == 0:\n",
    "                print('chekpoint', counter)\n",
    "#                 if counter > 2:\n",
    "#                     break\n",
    "\n",
    "            skt = loaded_SKT[f]\n",
    "            dcs = loaded_DCS[f]\n",
    "            if skt != None:\n",
    "                result, runDetails = algo.predict(skt, dcs, verbose = True)\n",
    "                if result == None:\n",
    "                    no_query += 1\n",
    "#                     SeeSentence(skt)\n",
    "                    continue\n",
    "                (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain) = SentencePreprocess(skt)\n",
    "                sol, solNoPvb = GetSolutions(dcs)\n",
    "#                 print(runDetails.keys())\n",
    "# #                 SeeSentence(skt)\n",
    "#                 display(runDetails['DCSLemmas'])\n",
    "#                 print('STEPS:', int(runDetails['steps']))\n",
    "#                 print(eval(runDetails['initialQuery']))\n",
    "                for step in range(int(runDetails['steps'])):\n",
    "                    stepData = runDetails[str(step)]\n",
    "#                     print(stepData['final_rank'])\n",
    "                    for ri in range(np.where(stepData['winner'][0] == stepData['final_rank'])[0][0], len(stepData['final_rank'])):\n",
    "#                         print(len(stepData['final_rank']))\n",
    "                        w = stepData['final_rank'][ri]\n",
    "#                         print(lemmaList[w] in sol,lemmaList[w] in solNoPvb , lemmaList[w])\n",
    "                        bcsv.writerow([stepData['w2w_score'][0,w], stepData['t2t_score'][0,w], stepData['w2w_samecng_score'][0,w], lemmaList[w] in sol or lemmaList[w] in solNoPvb])\n",
    "            else:\n",
    "                no_query += 1\n",
    "\n",
    "    print('Can\\'t form query node in', no_query, 'files')\n",
    "GetLogReg_TrainData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKT ANALYZE\n",
      "---------------\n",
      "priyApriye\n",
      "Analyzing  priyApriye\n",
      "0 :  priya ['prya'] [{'compound': ['iic.']}]\n",
      "0 :  priyA ['prya'] [{'noun': ['nom. sg. f.']}]\n",
      "4 :  Apriye ['AprI'] [{'noun': ['dat. sg. f.']}]\n",
      "4 :  apriye ['pf', 'pf'] [{'verb': ['impft. ps. sg. 1']}, {'verb': ['impft. ps. sg. 1']}]\n",
      "4 :  a ['a'] [{'compound': ['iic.']}]\n",
      "5 :  priye ['prya'] [{'noun': ['loc. sg. m.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'acc. du. f.', 'nom. du. f.']}]\n",
      "5 :  priye ['prya'] [{'noun': ['voc. du. n.', 'voc. du. f.', 'voc. sg. f.']}]\n",
      "5 :  priye ['pf', 'pf'] [{'verb': ['pr. ps. sg. 1']}, {'verb': ['pr. ps. sg. 1']}]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
