{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from AlgoTestFactory import *\n",
    "\n",
    "# altf1 = AlgoTestFactory([0, 10000], 15, savePath=\"Combined_4_test2\", storeAccuracies=True)\n",
    "\n",
    "# altf1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader Started[Prob]...\n",
      "Dataloader Finished[Prob]...\n"
     ]
    }
   ],
   "source": [
    "from SktWsegRWR_utf8 import *\n",
    "import pickle\n",
    "import ProbData\n",
    "from ProbModels import *\n",
    "import multiprocessing\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "import csv\n",
    "from utilities import *\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pb = ProbModels(fullCo_oc_mat = ProbData.fullCo_oc_mat, unigram_counts = ProbData.unigram_counts,\n",
    "               cng2cngFullMat = ProbData.cng2cngFullMat, cng2index_dict = ProbData.cng2index_dict,\n",
    "               w2w_samecng_fullmat=ProbData.w2w_samecng_fullmat, samecng_unigram_counts=ProbData.samecng_unigram_counts,\n",
    "               v2c_fullMat = ProbData.v2c_fullMat)\n",
    "\n",
    "goodFileDict = pickle.load(open('mergedGood_v5.p', 'rb'))\n",
    "algo = SktWsegRWR(\n",
    "            w2w_modelFunc = pb.get_w2w_mat, \n",
    "            t2t_modelFunc = pb.get_cng2cng_mat,\n",
    "            v2c_modelFunc = pb.get_v2c_ranking,\n",
    "            sameCng_modelFunc = pb.get_w2w_samecng_mat,\n",
    "            partition=[0.25, 0.25, 0.25, 0.1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49441"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pf1 = pd.read_csv('extras/pvbnhi@pf.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "# df_pf2 = pd.read_csv('extras/pvbnhi@skt.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "# df_pf3 = pd.read_csv('extras/pvbnhi@upd.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "\n",
    "# df_pf1['miss'] = df_pf1[df_pf1['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "# df_pf1 = df_pf1[df_pf1['miss'] == 0]\n",
    "# df_pf1['folder'] = '../TextSegmentation/Pickle_Files/'\n",
    "\n",
    "# df_pf2['miss'] = df_pf2[df_pf2['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "# df_pf2 = df_pf2[df_pf2['miss'] == 0]\n",
    "# df_pf2['folder'] = '../TextSegmentation/corrected_10to20/'\n",
    "\n",
    "# df_pf3['miss'] = df_pf3[df_pf3['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "# df_pf3 = df_pf3[df_pf3['miss'] == 0]\n",
    "# df_pf3['folder'] = '../TextSegmentation/Updated Pickles/'\n",
    "\n",
    "# frames = [df_pf1, df_pf2, df_pf3]\n",
    "# df_pf = pd.concat(frames)\n",
    "# %reset_selective df_pf1\n",
    "# %reset_selective df_pf2\n",
    "# %reset_selective df_pf3\n",
    "# goodFileDict = {}\n",
    "# for index, row in df_pf.iterrows():\n",
    "#     goodFileDict['%d.p' % row['file']] = '%s%d.p' % (row['folder'], row['file'])\n",
    "len(goodFileDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49441"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "[-0.3659 -1.8959 -2.8059  0.1176]\n",
      "13\n",
      "0 1 3\n",
      "[-0.3659 -1.9459 -2.9559  0.1176]\n",
      "17\n",
      "0 5 3\n",
      "[-0.3659 -2.1959 -3.1059  0.1176]\n",
      "25\n",
      "0 5 4\n",
      "[-0.3659 -2.4459 -3.3059  0.1176]\n",
      "26\n",
      "1 1 1\n",
      "[-0.4159 -2.4959 -3.3559  0.1176]\n",
      "21\n",
      "31031.p \t 66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "fs = ['31031.p']\n",
    "np.set_printoptions(precision=4)\n",
    "# for f in list(goodFileDict.keys())[1:20]:\n",
    "# for f in list(goodFileDict.keys())[1:100]:\n",
    "for f in fs:\n",
    "#     print('=='*20)\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result = algo.predict(sentenceObj, dcsObj, verbose = False, eta=0.05)\n",
    "        if(result != None):\n",
    "            solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "            ac = Accuracy(result, dcsObj)\n",
    "            print(f, \"\\t\", ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKT ANALYZE\n",
      "---------------\n",
      "na hi kAlopapannena dAnaviGnaH  kfto'rTinA   \n",
      "Analyzing  na\n",
      "0 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "Analyzing  hi\n",
      "0 :  hi ['h_1'] [{'indeclinable': ['part.']}]\n",
      "0 :  hi ['hi'] []\n",
      "Analyzing  kAlopapannena\n",
      "0 :  kAla ['kāla_1', 'kāla_2'] [{'compound': ['iic.']}, {'compound': ['iic.']}]\n",
      "0 :  kA ['km'] [{'noun': ['nom. sg. f.']}]\n",
      "0 :  kAla ['kāla'] []\n",
      "1 :  ala ['ala'] [{'compound': ['iic.']}]\n",
      "1 :  Ala ['āla'] [{'compound': ['iic.']}]\n",
      "1 :  a ['a'] [{'compound': ['iic.']}]\n",
      "1 :  upapad ['upapad'] []\n",
      "2 :  lopa ['lopa'] [{'compound': ['iic.']}]\n",
      "3 :  upapannena ['panna', 'pad_1'] [{'noun': ['i. sg. n.', 'i. sg. m.']}, {'verb': ['pp.']}]\n",
      "3 :  upapanna ['panna', 'pad_1'] [{'compound': ['iic.']}, {'verb': ['pp.']}]\n",
      "3 :  upapanne ['panna', 'pad_1'] [{'noun': ['acc. du. f.', 'nom. du. f.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'loc. sg. m.']}, {'verb': ['pp.']}]\n",
      "3 :  upapannA ['panna', 'pad_1'] [{'noun': ['nom. sg. f.']}, {'verb': ['pp.']}]\n",
      "6 :  pannena ['panna', 'pad_1'] [{'noun': ['i. sg. n.', 'i. sg. m.']}, {'verb': ['pp.']}]\n",
      "6 :  panna ['panna', 'pad_1'] [{'compound': ['iic.']}, {'verb': ['pp.']}]\n",
      "6 :  pannA ['panna', 'pad_1'] [{'noun': ['nom. sg. f.']}, {'verb': ['pp.']}]\n",
      "6 :  panne ['panna', 'pad_1'] [{'noun': ['acc. du. f.', 'nom. du. f.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'loc. sg. m.']}, {'verb': ['pp.']}]\n",
      "6 :  pam ['pa_1', 'pa_2'] [{'noun': ['acc. sg. m.']}, {'noun': ['acc. sg. m.']}]\n",
      "9 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "10 :  ina ['na'] [{'noun': ['voc. sg. m.', 'voc. sg. n.']}]\n",
      "11 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "Analyzing  dAnaviGnaḥ\n",
      "0 :  dAna ['dāna'] [{'compound': ['iic.']}]\n",
      "0 :  dAn ['dā_1', 'dā_2'] [{'noun': ['nom. sg. m.']}, {'verb': ['ppr. [2] ac.']}]\n",
      "3 :  avi ['av'] [{'compound': ['iic.']}]\n",
      "3 :  a ['a'] [{'compound': ['iic.']}]\n",
      "4 :  viGnaḥ ['vighna'] [{'noun': ['nom. sg. m.']}]\n",
      "6 :  Gnaḥ ['ghna', 'han_2'] [{'noun': ['nom. sg. m.']}, {'noun': ['acc. pl. m.', 'g. sg. m.', 'abl. sg. m.']}]\n",
      "Analyzing  kfto'rTinA\n",
      "0 :  kftaḥ ['kṛta', 'kṛ_1'] [{'noun': ['nom. sg. m.']}, {'verb': ['pp.']}]\n",
      "0 :  kf ['kṛ'] []\n",
      "4 :  arTinA ['arthin'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "\n",
      "DCS ANALYZE\n",
      "---------------\n",
      "na hi kAlopapannena dAnaviGnaH  kfto'rTinA   \n",
      "[['na'], ['hi'], ['kāla', 'upapad'], ['dāna', 'vighna'], ['kṛ', 'arthin']]\n",
      "Lemmas: ['na', 'hi', 'kAla', 'upapad', 'dAna', 'viGna', 'kf', 'arTin']\n",
      "[['2'], ['2'], ['3', '-190'], ['3', '29'], ['-190', '89']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = '30240.p'\n",
    "# print(goodFileDict[f])\n",
    "s, d = loadSentence(f, '../TextSegmentation/Updated Pickles/30240.p')\n",
    "SeeSentence(s)\n",
    "SeeDCS(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKT ANALYZE\n",
      "---------------\n",
      "nAmaBistu samAKyAtA nAradena mahAtmanA   \n",
      "Analyzing  nAmaBistu\n",
      "0 :  nAmaBiḥ ['nAman'] [{'noun': ['i. pl. n.']}]\n",
      "7 :  tu ['tu'] [{'indeclinable': ['conj.']}]\n",
      "Analyzing  samAKyAtA\n",
      "0 :  sama ['sama'] [{'compound': ['iic.']}]\n",
      "0 :  samA ['samA'] [{'compound': ['iic.']}]\n",
      "0 :  samA ['samA', 'sama'] [{'noun': ['nom. sg. f.']}, {'noun': ['nom. sg. f.']}]\n",
      "0 :  sa ['tad'] [{'noun': ['nom. sg. m.']}]\n",
      "0 :  samAKyA ['samAKyA'] [-190]\n",
      "2 :  mA ['mA'] [{'indeclinable': ['part.']}]\n",
      "2 :  mA ['mad'] [{'noun': ['acc. sg. *']}]\n",
      "3 :  AKyAtAḥ ['KyAta', 'KyA'] [{'noun': ['acc. pl. f.', 'nom. pl. f.', 'nom. pl. m.']}, {'verb': ['pp.']}]\n",
      "3 :  AKyAtA ['KyA', 'KyAta', 'KyA'] [{'verb': [['pp.']], 'noun': ['i. sg. n.', 'i. sg. m.'], 'verbform': ['\"http:sanskrit.inria.frMW75.html#H_khyaa\"']}, {'verb': ['ppr. [2] ac.', ['pp.']], 'verbform': ['\"http:sanskrit.inria.frMW75.html#H_khyaata\"']}, {'verb': [['pp.']], 'noun': ['nom. sg. f.'], 'verbform': ['\"http:sanskrit.inria.frMW75.html#H_khyaa\"']}]\n",
      "3 :  AKyAtA ['KyA'] [{'verb': ['per. fut. ac. sg. 3']}]\n",
      "3 :  AKyA ['AKyA'] [{'noun': ['nom. sg. f.']}]\n",
      "3 :  a ['a'] [{'compound': ['iic.']}]\n",
      "4 :  KyAtAḥ ['KyAta', 'KyA'] [{'noun': ['acc. pl. f.', 'nom. pl. f.', 'nom. pl. m.']}, {'verb': ['pp.']}]\n",
      "4 :  KyAtA ['KyA', 'KyAta', 'KyA'] [{'verb': [['pp.']], 'noun': ['i. sg. n.', 'i. sg. m.'], 'verbform': ['\"http:sanskrit.inria.frMW75.html#H_khyaa\"']}, {'verb': ['ppr. [2] ac.', ['pp.']], 'verbform': ['\"http:sanskrit.inria.frMW75.html#H_khyaata\"']}, {'verb': [['pp.']], 'noun': ['nom. sg. f.'], 'verbform': ['\"http:sanskrit.inria.frMW75.html#H_khyaa\"']}]\n",
      "4 :  KyAtA ['KyA'] [{'verb': ['per. fut. ac. sg. 3']}]\n",
      "7 :  tAḥ ['tad'] [{'noun': ['acc. pl. f.', 'nom. pl. f.']}]\n",
      "Analyzing  nAradena\n",
      "1 :  nAradena ['nArada'] [{'noun': ['i. sg. m.']}]\n",
      "1 :  nArada ['nArada'] [{'compound': ['iic.']}]\n",
      "1 :  nArade ['nArada'] [{'noun': ['loc. sg. m.']}]\n",
      "1 :  nAra ['nAra'] [{'compound': ['iic.']}]\n",
      "1 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "2 :  ara ['ara'] [{'compound': ['iic.']}]\n",
      "2 :  Ara ['Ara'] [{'compound': ['iic.']}]\n",
      "2 :  a ['a'] [{'compound': ['iic.']}]\n",
      "3 :  radena ['rada'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "3 :  rada ['rada'] [{'compound': ['iic.']}]\n",
      "3 :  radA ['rada'] [{'noun': ['nom. sg. f.']}]\n",
      "3 :  rade ['rada'] [{'noun': ['loc. sg. m.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'acc. du. f.', 'nom. du. f.']}]\n",
      "5 :  dena ['da'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "5 :  dA ['da'] [{'noun': ['nom. sg. f.']}]\n",
      "5 :  de ['da'] [{'noun': ['loc. sg. m.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'acc. du. f.', 'nom. du. f.']}]\n",
      "5 :  da ['da'] [{'compound': ['iic.']}]\n",
      "6 :  ina ['na'] [{'noun': ['voc. sg. m.', 'voc. sg. n.']}]\n",
      "7 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "Analyzing  mahAtmanA\n",
      "0 :  mahA ['mahant'] [{'compound': ['iic.']}]\n",
      "0 :  maha ['mah'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "0 :  mahAtman ['mahAtman'] [89]\n",
      "3 :  AtmanA ['Atman'] [{'noun': ['i. sg. *', 'i. sg. m.']}]\n",
      "3 :  AtmanA ['Atman'] [{'noun': ['i. sg. n.']}]\n",
      "\n",
      "38838.p \t 40.0\n",
      "========================================\n",
      "[['nāman', 'tu'], ['samākhyā'], ['nārada'], ['mahātman']]\n",
      "['nAman', 'tu', 'AKyA', 'nArada', 'mahAtman']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nAman',\n",
       " 'tu',\n",
       " 'tad',\n",
       " 'tad',\n",
       " 'na',\n",
       " 'na',\n",
       " 'mad',\n",
       " 'mahant',\n",
       " 'Atman',\n",
       " 'da',\n",
       " 'a',\n",
       " 'a']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Query: [0, 1]\n",
      "------------------------------\n",
      "Step: 0\n",
      "------------------------------\n",
      "Winner: (6, 'sa', 'tad', 29)\n",
      "Removed: [(2, 'sama', 'sama', 3), (3, 'samA', 'samA', 3), (4, 'samA', 'samA', 30), (5, 'samA', 'sama', 30), (7, 'samAKyA', 'samAKyA', -190)]\n",
      "Query: [0, 1, 6]\n",
      "------------------------------\n",
      "Step: 1\n",
      "------------------------------\n",
      "Winner: (33, 'tAḥ', 'tad', 40)\n",
      "Removed: [(32, 'tAḥ', 'tad', 80), (10, 'AKyAtAḥ', 'KyAta', 80), (11, 'AKyAtAḥ', 'KyAta', 40), (12, 'AKyAtAḥ', 'KyAta', 39), (13, 'AKyAtAḥ', 'KyA', -190), (14, 'AKyAtA', 'KyA', 89), (15, 'AKyAtA', 'KyA', 91), (16, 'AKyAtA', 'KyAta', -190), (17, 'AKyAtA', 'KyAta', -10), (18, 'AKyAtA', 'KyA', 30), (19, 'AKyAtA', 'KyA', -73), (22, 'KyAtAḥ', 'KyAta', 80), (23, 'KyAtAḥ', 'KyAta', 40), (24, 'KyAtAḥ', 'KyAta', 39), (25, 'KyAtAḥ', 'KyA', -190), (26, 'KyAtA', 'KyA', 89), (27, 'KyAtA', 'KyA', 91), (28, 'KyAtA', 'KyAta', -190), (29, 'KyAtA', 'KyAta', -10), (30, 'KyAtA', 'KyA', 30), (31, 'KyAtA', 'KyA', -73)]\n",
      "Query: [0, 1, 6, 33]\n",
      "------------------------------\n",
      "Step: 2\n",
      "------------------------------\n",
      "Winner: (64, 'na', 'na', 2)\n",
      "Removed: [(34, 'nAradena', 'nArada', 89), (42, 'radena', 'rada', 89), (43, 'radena', 'rada', 91), (52, 'dena', 'da', 89), (53, 'dena', 'da', 91), (62, 'ina', 'na', 49), (63, 'ina', 'na', 51)]\n",
      "Query: [0, 1, 6, 33, 64]\n",
      "------------------------------\n",
      "Step: 3\n",
      "------------------------------\n",
      "Winner: (38, 'na', 'na', 2)\n",
      "Removed: [(35, 'nArada', 'nArada', 3), (36, 'nArade', 'nArada', 49), (37, 'nAra', 'nAra', 3)]\n",
      "Query: [0, 1, 6, 33, 64, 38]\n",
      "------------------------------\n",
      "Step: 4\n",
      "------------------------------\n",
      "Winner: (9, 'mA', 'mad', 72)\n",
      "Removed: [(8, 'mA', 'mA', 2)]\n",
      "Query: [0, 1, 6, 33, 64, 38, 9]\n",
      "------------------------------\n",
      "Step: 5\n",
      "------------------------------\n",
      "Winner: (65, 'mahA', 'mahant', 3)\n",
      "Removed: [(66, 'maha', 'mah', -32), (67, 'mahAtman', 'mahAtman', 89)]\n",
      "Query: [0, 1, 6, 33, 64, 38, 9, 65]\n",
      "------------------------------\n",
      "Step: 6\n",
      "------------------------------\n",
      "Winner: (68, 'AtmanA', 'Atman', 89)\n",
      "Removed: [(69, 'AtmanA', 'Atman', 92), (70, 'AtmanA', 'Atman', 91)]\n",
      "Query: [0, 1, 6, 33, 64, 38, 9, 65, 68]\n",
      "------------------------------\n",
      "Step: 7\n",
      "------------------------------\n",
      "Winner: (61, 'da', 'da', 3)\n",
      "Removed: [(44, 'rada', 'rada', 3), (45, 'radA', 'rada', 30), (46, 'rade', 'rada', 35), (47, 'rade', 'rada', 36), (48, 'rade', 'rada', 75), (49, 'rade', 'rada', 76), (50, 'rade', 'rada', 49), (51, 'rade', 'rada', 51), (54, 'dA', 'da', 30), (55, 'de', 'da', 35), (56, 'de', 'da', 36), (57, 'de', 'da', 75), (58, 'de', 'da', 76), (59, 'de', 'da', 49), (60, 'de', 'da', 51)]\n",
      "Query: [0, 1, 6, 33, 64, 38, 9, 65, 68, 61]\n",
      "------------------------------\n",
      "Step: 8\n",
      "------------------------------\n",
      "Winner: (41, 'a', 'a', 3)\n",
      "Removed: [(39, 'ara', 'ara', 3), (40, 'Ara', 'Ara', 3)]\n",
      "Query: [0, 1, 6, 33, 64, 38, 9, 65, 68, 61, 41]\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "fs = [ '38838.p']\n",
    "def debugSentences(fs):\n",
    "    np.set_printoptions(precision=4)\n",
    "    # for f in list(goodFileDict.keys())[1:20]:\n",
    "    # for f in list(goodFileDict.keys())[1:100]:\n",
    "    for f in fs:\n",
    "    #     print('=='*20)\n",
    "        sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "        SeeSentence(sentenceObj)\n",
    "        if(sentenceObj != None):\n",
    "            result, detail = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "            if(result != None):\n",
    "                solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "                ac = Accuracy(result, dcsObj)\n",
    "                print(f, \"\\t\", ac)\n",
    "#                 display(detail['sentence'])\n",
    "#                 display(detail['DCSLemmas'])\n",
    "                solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "                solution_no_pvb = [removePrefix(l) for l in solution]\n",
    "                print('='*40)\n",
    "                print(dcsObj.lemmas)\n",
    "                print(solution_no_pvb)\n",
    "                display(result)\n",
    "#                 print(detail['nodeList'])\n",
    "                print('Initial Query:', detail['initialQuery'])\n",
    "                for step in range(detail['steps']):\n",
    "                    print('-'*30)\n",
    "                    print('Step:', step)\n",
    "                    print('-'*30)\n",
    "                    print('Winner:', detail[str(step)]['winner'])\n",
    "                    print('Removed:', detail[str(step)]['removed'])\n",
    "                    print('Query:', detail[str(step)]['updated_query'])\n",
    "        print('='*30)\n",
    "\n",
    "debugSentences(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40559.p \t 85.71428571428571\n",
      "9882.p \t 57.142857142857146\n",
      "9322.p \t 70.0\n",
      "46252.p \t 42.857142857142854\n",
      "5914.p \t 28.571428571428573\n",
      "8100.p \t 84.61538461538461\n",
      "20991.p \t 75.0\n",
      "9956.p \t 87.5\n",
      "30191.p \t 42.857142857142854\n",
      "1965.p \t 50.0\n",
      "33847.p \t 57.142857142857146\n",
      "36166.p \t 66.66666666666667\n",
      "434717.p \t 80.0\n",
      "121160.p \t 77.77777777777777\n",
      "416689.p \t 50.0\n",
      "155344.p \t 83.33333333333333\n",
      "25556.p \t 87.5\n",
      "12103.p \t 57.142857142857146\n",
      "18601.p \t 85.71428571428571\n",
      "148399.p \t 62.5\n",
      "44388.p \t 50.0\n",
      "40786.p \t 70.0\n",
      "37439.p \t 71.42857142857143\n",
      "258081.p \t 85.71428571428571\n",
      "5115.p \t 80.0\n",
      "10770.p \t 66.66666666666667\n",
      "34794.p \t 75.0\n",
      "38614.p \t 42.857142857142854\n",
      "1580.p \t 100.0\n",
      "187398.p \t 66.66666666666667\n",
      "17538.p \t 83.33333333333333\n",
      "27724.p \t 77.77777777777777\n",
      "163.p \t 33.333333333333336\n",
      "22987.p \t 85.71428571428571\n",
      "45085.p \t 87.5\n",
      "16923.p \t 100.0\n",
      "50833.p \t 87.5\n",
      "4509.p \t 60.0\n",
      "337386.p \t 100.0\n",
      "222364.p \t 71.42857142857143\n",
      "139573.p \t 100.0\n",
      "9889.p \t 50.0\n",
      "36265.p \t 57.142857142857146\n",
      "228536.p \t 42.857142857142854\n",
      "34139.p \t 85.71428571428571\n",
      "12414.p \t 88.88888888888889\n",
      "59819.p \t 87.5\n",
      "260262.p \t 85.71428571428571\n",
      "304781.p \t 100.0\n",
      "133203.p \t 16.666666666666668\n",
      "42612.p \t 71.42857142857143\n",
      "6117.p \t 62.5\n",
      "14954.p \t 71.42857142857143\n",
      "309202.p \t 100.0\n",
      "439519.p \t 87.5\n",
      "281290.p \t 66.66666666666667\n",
      "4090.p \t 80.0\n",
      "416947.p \t 75.0\n",
      "2755.p \t 50.0\n",
      "25618.p \t 85.71428571428571\n",
      "103270.p \t 71.42857142857143\n",
      "29889.p \t 66.66666666666667\n",
      "27856.p \t 85.71428571428571\n",
      "15220.p \t 50.0\n",
      "422997.p \t 66.66666666666667\n",
      "174984.p \t 66.66666666666667\n",
      "166666.p \t 87.5\n",
      "299336.p \t 75.0\n",
      "45142.p \t 71.42857142857143\n",
      "8391.p \t 71.42857142857143\n",
      "16644.p \t 54.54545454545455\n",
      "16612.p \t 85.71428571428571\n",
      "58907.p \t 57.142857142857146\n",
      "37483.p \t 64.86486486486487\n",
      "90675.p \t 87.5\n",
      "1739.p \t 71.42857142857143\n",
      "19535.p \t 57.142857142857146\n",
      "25790.p \t 87.5\n",
      "137499.p \t 71.42857142857143\n",
      "11355.p \t 57.142857142857146\n",
      "37853.p \t 66.66666666666667\n",
      "27209.p \t 75.0\n",
      "29207.p \t 75.0\n",
      "315541.p \t 88.88888888888889\n"
     ]
    }
   ],
   "source": [
    "# with open('.temp/sandhi_encounters.csv', 'w') as fh:\n",
    "#     fcsv = csv.writer(fh)\n",
    "#     fcsv.writerow(['left', 'right', 'derivations', 'word_left', 'word_right', 'pos_left', 'pos_right'])\n",
    "# np.set_printoptions(precision=6)\n",
    "# for f in list(goodFileDict.keys())[90:91]:\n",
    "for f in list(goodFileDict.keys())[0:100]:\n",
    "# for f in fullfs:\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result = algo.predict(sentenceObj, dcsObj, eta = 0.05)\n",
    "        if(result != None):\n",
    "            ac = Accuracy(result, dcsObj)\n",
    "            solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "#             print(result)\n",
    "#             print('vs')\n",
    "#             print(solution)\n",
    "\n",
    "            print(f, \"\\t\", ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fullfs = ['105587.p','293473.p','379245.p']\n",
    "fullfs = ['302864.p', '292204.p', '161425.p', '128533.p', '265112.p', '16349.p', '320178.p', '334655.p', '331468.p', '365975.p', '434538.p', '148267.p', '340997.p', '204751.p', '289589.p', '423330.p', '276728.p', '105056.p', '325962.p', '383515.p', '60055.p', '228316.p', '192549.p', '169215.p', '393452.p', '433996.p', '440103.p', '68686.p', '157189.p', '440567.p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowscore files found: 4\n",
      "Midscore files found: 7\n",
      "Highscore files found: 9\n"
     ]
    }
   ],
   "source": [
    "fullfs = ['302864.p', '292204.p', '161425.p', '128533.p', '265112.p', '16349.p', '320178.p', '334655.p', '331468.p', '365975.p', '434538.p', '148267.p', '340997.p', '204751.p', '289589.p', '423330.p', '276728.p', '105056.p', '325962.p', '383515.p', '60055.p', '228316.p', '192549.p', '169215.p', '393452.p', '433996.p', '440103.p', '68686.p', '157189.p', '440567.p']\n",
    "def GetSomeJSON():\n",
    "    lowScore = []\n",
    "    midScore = []\n",
    "    highScore = []\n",
    "    np.set_printoptions(precision=4)\n",
    "#     for i in range(1, 400):\n",
    "#         f = list(goodFileDict.keys())[i]\n",
    "#     print(len(fullfs))\n",
    "    for f in fullfs:\n",
    "        sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "        if(sentenceObj != None):\n",
    "            result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "\n",
    "            if(result != None):\n",
    "                runDetails['file'] = f\n",
    "                ac = runDetails['accuracy']\n",
    "#                 print(f, ac)\n",
    "\n",
    "                # CODE FOR SELECTING FILES \n",
    "#                 if(i%100 == 0):\n",
    "#                     print('Chekpoint:', i)\n",
    "                if(ac <= 40):\n",
    "                    lowScore.append(runDetails)\n",
    "                elif (ac <= 60):\n",
    "                    if(ac >= 45):\n",
    "                        midScore.append(runDetails)\n",
    "                elif (ac <= 100):\n",
    "                    if(ac >= 80):\n",
    "                        highScore.append(runDetails)\n",
    "\n",
    "    print('Lowscore files found:', len(lowScore))\n",
    "    print('Midscore files found:', len(midScore))\n",
    "    print('Highscore files found:', len(highScore))\n",
    "\n",
    "    lowScore = lowScore[0:10]\n",
    "    midScore = midScore[0:10]\n",
    "    highScore = highScore[0:10]\n",
    "    pickle.dump(lowScore, open('lowScore.p', 'wb'))\n",
    "    pickle.dump(midScore, open('midScore.p', 'wb'))\n",
    "    pickle.dump(highScore, open('highScore.p', 'wb'))\n",
    "\n",
    "#=================================================================\n",
    "GetSomeJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chekpoint: 0\n",
      "Chekpoint: 100\n",
      "Chekpoint: 300\n"
     ]
    }
   ],
   "source": [
    "def MacroAccuracy():\n",
    "    allAcs = []\n",
    "    np.set_printoptions(precision=4)\n",
    "    gfs = list(goodFileDict.keys())\n",
    "    for i in range(400):\n",
    "#     for i in range(10):\n",
    "        f = gfs[i]\n",
    "        sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "        if(sentenceObj != None):\n",
    "            result = algo.predict(sentenceObj, dcsObj, verbose = False)\n",
    "            \n",
    "            if(result != None):\n",
    "#                 print(result)\n",
    "#                 print(dcsObj.lemmas)\n",
    "                ac = Accuracy(result, dcsObj)\n",
    "                allAcs.append(ac)\n",
    "#                 print(f, ac)\n",
    "\n",
    "                # CODE FOR SELECTING FILES \n",
    "                if(i%100 == 0):\n",
    "                    print('Chekpoint:', i)\n",
    "    allAcs = np.array(allAcs)\n",
    "    return allAcs\n",
    "\n",
    "\n",
    "#=================================================================\n",
    "allAcs = MacroAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "76.8498538275\n",
      "Macro Accuracy %: 20.6896551724\n"
     ]
    }
   ],
   "source": [
    "print(len(allAcs))\n",
    "print(np.mean(allAcs))\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(allAcs, 40, normed=False)\n",
    "\n",
    "plt.xlabel('MicroAccuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of MicroAccuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('Macro Accuracy %:', 100*np.sum(n[(bins[::] > 95)[:-1:]])/np.sum(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SeeSomeJSON(region = 'mid', files = ['lowScore.p', 'midScore.p', 'highScore.p']):\n",
    "    np.set_printoptions(precision=3)\n",
    "    if region == 'low':\n",
    "        p = pickleFixLoad(files[0])\n",
    "    elif region == 'mid':\n",
    "        p = pickleFixLoad(files[1])\n",
    "    else:\n",
    "        p = pickleFixLoad(files[2])\n",
    "#     for pf in p:\n",
    "#         print(pf['file'])\n",
    "    n_steps = p[0]['steps']\n",
    "    for s in range(n_steps):\n",
    "        \n",
    "        print('Step:', s)\n",
    "        print('='*20)\n",
    "        \n",
    "        for key, val in p[0][str(s)].items():\n",
    "            print(key, \":\")\n",
    "            print(val)\n",
    "            print('-'*20)\n",
    "        \n",
    "SeeSomeJSON('low')\n",
    "print('='*20)\n",
    "SeeSomeJSON('mid')\n",
    "print('='*20)\n",
    "SeeSomeJSON('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "35 weird predictions encountered.\n"
     ]
    }
   ],
   "source": [
    "def ArePredictionsOK():\n",
    "    with open('.temp/weird.csv', 'w') as fh:\n",
    "        wcsv = csv.writer(fh)\n",
    "        wcsv.writerow(['filename', 'sentence', 'solution', 'prediction'])\n",
    "        np.set_printoptions(precision=4)\n",
    "        badCount = 0\n",
    "        for i in range(1, 600):\n",
    "            f = list(goodFileDict.keys())[i]\n",
    "\n",
    "            sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "            if(sentenceObj != None):\n",
    "                result = algo.predict(sentenceObj, dcsObj, verbose = False)\n",
    "                if(result != None):            \n",
    "                    solution = [rom_slp(c) for arr in dcsObj.lemmas for c in arr]\n",
    "                    ac = Accuracy(result, dcsObj)\n",
    "                    if ac >= 100 and len(result) != len(solution):\n",
    "                        badCount += 1\n",
    "                        if(badCount%10 == 0):\n",
    "                            print(\"=\"*40)\n",
    "                        sline = [f, sentenceObj.sentence, solution, result]\n",
    "                        wcsv.writerow(sline)\n",
    "        print(badCount, 'weird predictions encountered.')\n",
    "        \n",
    "ArePredictionsOK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKT ANALYZE\n",
      "---------------\n",
      "paYca ratnAni muKyAni coparatnacatuzwayam   \n",
      "Analyzing: paYca\n",
      "-------------------------\n",
      "Analyzing: ratnAni\n",
      "0 : ratnAni [0, 'ratna', 81] [1, 'ratna', 41] \n",
      "0 : ratnAni [2, 'ratna', 61] \n",
      "-------------------------\n",
      "Analyzing: muKyAni\n",
      "0 : muKyAni [3, 'muKya', 81] [4, 'muKya', 41] \n",
      "0 : muKyAni [5, 'muKya', 61] \n",
      "-------------------------\n",
      "Analyzing: coparatnacatuzwayam\n",
      "0 : ca [6, 'ca', 2] \n",
      "9 : catuzwayam [7, 'catuzwaya', 31] [8, 'catuzwaya', 69] [9, 'catuzwaya', 71] \n",
      "4 : ratna [10, 'ratna', 3] \n",
      "1 : uparatna [11, 'uparatna', 3] \n",
      "1 : Upa [12, 'vap', -158] \n",
      "-------------------------\n",
      "Analyzing  paYca\n",
      "0 :  paYca [] []\n",
      "Analyzing  ratnAni\n",
      "0 :  ratnAni ['ratna'] [{'noun': ['acc. pl. n.', 'nom. pl. n.']}]\n",
      "0 :  ratnAni ['ratna'] [{'noun': ['voc. pl. n.']}]\n",
      "Analyzing  muKyAni\n",
      "0 :  muKyAni ['muKya'] [{'noun': ['acc. pl. n.', 'nom. pl. n.']}]\n",
      "0 :  muKyAni ['muKya'] [{'noun': ['voc. pl. n.']}]\n",
      "Analyzing  coparatnacatuzwayam\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "9 :  catuzwayam ['catuzwaya'] [{'noun': ['acc. sg. m.', 'acc. sg. n.', 'nom. sg. n.']}]\n",
      "4 :  ratna ['ratna'] [{'compound': ['iic.']}]\n",
      "1 :  uparatna ['uparatna'] [{'compound': ['iic.']}]\n",
      "1 :  Upa ['vap', 'vap'] [{'verb': ['pft. ac. pl. 2']}, {'verb': ['pft. ac. pl. 2']}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceObj, dcsObj = loadSentence('1.p', '../TextSegmentation/corrected_10to20/1.p')\n",
    "SeeSentence(sentenceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCS ANALYZE\n",
      "---------------\n",
      "upacAreRa SIlena rUpayOvanasaMpadA   \n",
      "[['upacāra'], ['śīla'], ['rūpa', 'yauvana', 'sampad']]\n",
      "Lemmas: ['upacAra', 'SIla', 'rUpa', 'yOvana', 'sampad']\n",
      "[['89'], ['91'], ['3', '3', '90']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SeeDCS(dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CSVforSupervised():\n",
    "    np.set_printoptions(precision=4)\n",
    "    # Clean the csv files\n",
    "    metPerfFH = open('.temp/metPerfFH.csv', 'w')\n",
    "    metPerfFH_bin = open('.temp/metPerfFH_bin.csv', 'w')\n",
    "\n",
    "    metPerfFH.write('w2w,t2t,w2w_samecng,InDCS\\n')\n",
    "    metPerfFH_bin.write('w2w,t2t,w2w_samecng,InDCS\\n')\n",
    "\n",
    "    metPerfFH.close()\n",
    "    metPerfFH_bin.close()\n",
    "    ######################\n",
    "    gfl = list(goodFileDict.keys())\n",
    "    for i in range(0, 11000):\n",
    "        f = gfl[i]\n",
    "        if(i%250 == 0):\n",
    "            print('Checkpoint:', i)\n",
    "        sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "        \n",
    "        if(sentenceObj != None):\n",
    "            result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6)\n",
    "jsonBatch = []\n",
    "for f_ in lowScore[0:10]:\n",
    "    f = f_[0]\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "        runDetails['file'] = f\n",
    "        jsonBatch.append(runDetails)\n",
    "pickle.dump(jsonBatch, open('.temp/lowScore.p','wb'))\n",
    "print('33 % Complete')\n",
    "#################################################################\n",
    "jsonBatch = []\n",
    "for f_ in midScore[0:10]:\n",
    "    f = f_[0]\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "        runDetails['file'] = f\n",
    "        jsonBatch.append(runDetails)\n",
    "pickle.dump(jsonBatch, open('.temp/midScore.p','wb'))\n",
    "print('67 % Complete')\n",
    "#################################################################\n",
    "jsonBatch = []\n",
    "for f_ in highScore[0:10]:\n",
    "    f = f_[0]\n",
    "    sentenceObj, dcsObj = loadSentence(f, goodFileDict[f])\n",
    "    if(sentenceObj != None):\n",
    "        result, runDetails = algo.predict(sentenceObj, dcsObj, verbose = True)\n",
    "        runDetails['file'] = f\n",
    "        jsonBatch.append(runDetails)\n",
    "print(pprint.pprint(jsonBatch))\n",
    "pickle.dump(jsonBatch, open('.temp/highScore.p','wb'))\n",
    "print('100 % Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "a[:-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
