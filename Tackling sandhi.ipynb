{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from utilities import *\n",
    "from DCS import *\n",
    "from sentences import *\n",
    "from romtoslp import rom_slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sandhiRules = defaultdict(lambda: set())\n",
    "# sandhiConflicts = defaultdict(lambda: [])\n",
    "# with open('extras/sandhiRulesSLP.txt') as f:\n",
    "#     # 2611 lines\n",
    "#     for l in f.readlines():\n",
    "#         g = re.split('[ \\\"]*,[ \\\"]*', l)\n",
    "#         try:\n",
    "#             if ' ' not in g[3]:\n",
    "#                 p = (g[1], g[2])\n",
    "#                 sandhiRules[p].add(g[3])\n",
    "#         except IndexError:\n",
    "#             pass\n",
    "            \n",
    "# for key, vals in sandhiRules.items():\n",
    "#     if(len(vals) > 1):\n",
    "#         l = set()\n",
    "#         for derive in vals:\n",
    "#             l.add(len(derive))\n",
    "#             if(len(l) > 1):\n",
    "# #                 print(key, vals)\n",
    "#                 sandhiConflicts[key] = vals\n",
    "#                 break\n",
    "\n",
    "                \n",
    "# for key, val in sandhiConflicts.items():\n",
    "#     sandhiRules.pop(key)\n",
    "# for key, val in sandhiRules.items():\n",
    "#     sandhiRules[key] = {'derivations':list(val),\n",
    "#                         'length':len(list(val)[0])}\n",
    "# #     print(sandhiRules[key])\n",
    "# pickle.dump(dict(sandhiRules), open('extras/sandhiRules.p', 'wb'))\n",
    "sandhiRules = pickle.load(open('extras/sandhiRules.p','rb'))\n",
    "# for key, val in sandhiRules.items():\n",
    "#     print(key, val)\n",
    "#     if(len(key[0]) + len(key[1]) > val['length']):\n",
    "#         print(key, val)\n",
    "#     r = re.match('[^\\w]+', ''.join(val['derivations']))\n",
    "#     if r!=None:\n",
    "#         print(key, val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadSentence(fName, folderTag):\n",
    "        # print('File: ', fName)\n",
    "        try:\n",
    "            dcsObj = pickleFixLoad('../Text Segmentation/DCS_pick/' + fName)           \n",
    "            if folderTag == \"C1020\" :\n",
    "                sentenceObj = pickleFixLoad('../TextSegmentation/corrected_10to20/' + fName)\n",
    "            else:\n",
    "                sentenceObj = pickleFixLoad('../TextSegmentation/Pickle_Files/' + fName)\n",
    "\n",
    "        except (KeyError, EOFError, pickle.UnpicklingError) as e:\n",
    "            return None, None\n",
    "        return(sentenceObj, dcsObj)\n",
    "goodFiles = pickle.load(open('mergedGood_v3.p', 'rb'), encoding=u'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'a') ('a', 'a')\n",
      "rama avatar ('a', 'a')  =  {'length': 1, 'derivations': ['A']}\n",
      "True\n",
      "('aH', 'a') ('H', 'av')\n",
      "ramaH avatar ('aH', 'a')  =  {'length': 2, 'derivations': [\"o'\"]}\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def CanCoExist_simple(p1, p2, n1, n2):\n",
    "    # Make sure p1 is < p2, always\n",
    "    if(p1 < p2):\n",
    "        if(p1 + len(n1) - 1 < p2):\n",
    "            return True\n",
    "    return False\n",
    "def CanCoExist(p1, p2, name1, name2):\n",
    "    # P1 must be less than P2\n",
    "    # Just send it in the proper order\n",
    "    if(p1 < p2):\n",
    "        overlap = max((p1 + len(name1)) - p2, 0)\n",
    "        if overlap == 0:\n",
    "            return True\n",
    "        if overlap == 1 or overlap == 2:\n",
    "            p1 = (name1[len(name1) - overlap:len(name1):], name2[0])\n",
    "            p2 = (name1[-1], name2[0:overlap:])\n",
    "            print(p1, p2)\n",
    "            if p1 in sandhiRules:\n",
    "                print(name1, name2, p1, ' = ', sandhiRules[p1])\n",
    "                if(sandhiRules[p1]['length'] < len(p1[0]) + len(p1[1])):\n",
    "                    return True\n",
    "            if p2 in sandhiRules:\n",
    "                print(name1, name2, p2, ' = ', sandhiRules[p2])\n",
    "                if(sandhiRules[p2]['length'] < len(p2[0]) + len(p2[1])):\n",
    "                    return True\n",
    "    return False\n",
    "print(CanCoExist(0,3,'rama','avatar'))\n",
    "print(CanCoExist(0,3,'ramaH','avatar'))\n",
    "print(CanCoExist_simple(0,3,'olde','sava'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# for f in list(goodFiles.keys())[0:5]:\n",
    "#     sentenceObj, dcsObj = loadSentence(f, goodFiles[f])\n",
    "#     try:\n",
    "#         for chunk in sentenceObj.chunk:\n",
    "#             for pos1, words1 in chunk.chunk_words.items():\n",
    "#                 for pos2, words2 in chunk.chunk_words.items():\n",
    "#                     for word1 in words1:\n",
    "#                         for word2 in words2:\n",
    "#                             if(pos1 < pos2 and len(word1.names) + pos1 == pos2 + 1):\n",
    "#                                 n1 = rom_slp(word1.names)\n",
    "#                                 n2 = rom_slp(word2.names)\n",
    "#                                 p = (n1[-1], n2[0])\n",
    "#                                 if p in sandhiRules:\n",
    "#                                     print(n1, n2, p, ' = ', sandhiRules[p])\n",
    "#     except AttributeError:\n",
    "#         print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaM prayatnaM kurvIta yAnaSayyAsanASane   \n",
      "Analyzing  evam\n",
      "0 :  evam ['evam'] [{'undetermined': ['adv.']}]\n",
      "Analyzing  prayatnam\n",
      "0 :  prayatnam ['prayatna'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "Analyzing  kurvIta\n",
      "0 :  kurvIta ['kf_1'] [{'verb': ['opt. [8] md. sg. 3']}]\n",
      "Analyzing  yAnaSayyAsanASane\n",
      "0 :  yAna ['yAna'] [{'compound': ['iic.']}]\n",
      "0 :  yAs ['ya_1'] [{'noun': ['acc. pl. f.', 'nom. pl. f.']}]\n",
      "0 :  yAn ['ya_1'] [{'noun': ['acc. pl. m.']}]\n",
      "0 :  yA ['ya_1'] [{'noun': ['nom. sg. f.']}]\n",
      "1 :  Ana ['Ana'] [{'compound': ['iic.']}]\n",
      "1 :  Ana ['an_2'] [{'verb': ['pft. ac. pl. 2', 'pft. ac. sg. 3', 'pft. ac. sg. 1']}]\n",
      "1 :  an ['an_1'] [{'compound': ['iic.']}]\n",
      "2 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "3 :  aSayi ['SI_1'] [{'verb': ['impft. [2] md. sg. 1']}]\n",
      "3 :  a ['a'] [{'compound': ['iic.']}]\n",
      "4 :  SayyA ['SayyA'] [{'noun': ['nom. sg. f.']}]\n",
      "8 :  asana ['asana'] [{'compound': ['iic.']}]\n",
      "8 :  Asana ['Asana'] [{'compound': ['iic.']}]\n",
      "8 :  asanA ['asana'] [{'noun': ['nom. sg. f.']}]\n",
      "8 :  asa ['asan'] [{'compound': ['iic.']}]\n",
      "8 :  Asa ['Asan'] [{'compound': ['iic.']}]\n",
      "8 :  asa ['as_2'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "8 :  Asa ['as_1', 'as_2'] [{'verb': ['pft. ac. pl. 2', 'pft. ac. sg. 3', 'pft. ac. sg. 1']}, {'verb': ['pft. ac. pl. 2', 'pft. ac. sg. 3', 'pft. ac. sg. 1']}]\n",
      "8 :  a ['a'] [{'compound': ['iic.']}]\n",
      "9 :  sana ['sana_1', 'sana_2'] [{'compound': ['iic.']}, {'compound': ['iic.']}]\n",
      "9 :  sanA ['sana_2'] [{'noun': ['nom. sg. f.']}]\n",
      "9 :  sa ['tad'] [{'noun': ['nom. sg. m.']}]\n",
      "11 :  nASane ['nASana'] [{'noun': ['loc. sg. m.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.']}]\n",
      "11 :  nASane ['nASana'] [{'noun': ['voc. du. n.']}]\n",
      "11 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "12 :  aSane ['aSana_1', 'aSana_2'] [{'noun': ['loc. sg. m.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'acc. du. f.', 'nom. du. f.']}, {'noun': ['acc. du. n.', 'nom. du. n.', 'loc. sg. n.']}]\n",
      "12 :  aSane ['aSana_2'] [{'noun': ['acc. du. f.', 'nom. du. f.', 'loc. sg. m.']}]\n",
      "12 :  aSane ['aSana_1', 'aSana_2'] [{'noun': ['voc. du. n.', 'voc. du. f.', 'voc. sg. f.']}, {'noun': ['voc. du. n.']}]\n",
      "12 :  a ['a'] [{'compound': ['iic.']}]\n",
      "13 :  Sane ['San'] [{'noun': ['voc. sg. m.']}]\n"
     ]
    }
   ],
   "source": [
    "SeeSentence(sentenceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaM prayatnaM kurvIta yAnaSayyAsanASane   \n",
      "[['evam'], ['prayatna'], ['kṛ'], ['yāna', 'śayyā', 'āsana', 'aśana']]\n",
      "Lemmas: ['evam', 'prayatna', 'kf', 'yAna', 'SayyA', 'Asana', 'aSana']\n",
      "[['2'], ['69'], ['-23'], ['3', '3', '3', '171']]\n"
     ]
    }
   ],
   "source": [
    "SeeDCS(dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('N', 'S') {'NKS', 'NS', 'NkS'}\n",
      "('e', 'C') {'eC', 'ecC'}\n",
      "('n', 's') {'nts', 'ns'}\n",
      "('E', 'C') {'EC', 'EcC'}\n",
      "('A', 'C') {'AC', 'AcC'}\n",
      "('I', 'C') {'IC', 'IcC'}\n",
      "('R', 'S') {'RS', 'RwS', 'RWS'}\n",
      "('U', 'C') {'UC', 'UcC'}\n",
      "('R', 's') {'Rws', 'RWs', 'Rs'}\n",
      "('N', 's') {'Nks', 'Ns', 'NKs'}\n",
      "('n', 'S') {'YS', 'YcS'}\n",
      "('O', 'C') {'OcC', 'OC'}\n",
      "('F', 'C') {'FC', 'FcC'}\n",
      "('o', 'C') {'oC', 'ocC'}\n",
      "('N', 'z') {'Nz', 'NKz', 'Nkz'}\n",
      "('w', 's') {'wts', 'ws'}\n",
      "('R', 'z') {'Rz', 'RWz', 'Rwz'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('F', 'C') ['FcC', 'FC']\n",
      "('n', 'S') ['YcS', 'YS']\n",
      "('R', 'z') ['RWz', 'Rz']\n",
      "('o', 'C') ['ocC', 'oC']\n",
      "('E', 'C') ['EcC', 'EC']\n",
      "('U', 'C') ['UcC', 'UC']\n",
      "('N', 'S') ['NKS', 'NS']\n",
      "('O', 'C') ['OcC', 'OC']\n",
      "('n', 's') ['nts', 'ns']\n",
      "('N', 's') ['NKs', 'Ns']\n",
      "('N', 'z') ['NKz', 'Nz']\n",
      "('A', 'C') ['AC', 'AcC']\n",
      "('w', 's') ['wts', 'ws']\n",
      "('R', 's') ['RWs', 'Rs']\n",
      "('R', 'S') ['RWS', 'RS']\n",
      "('I', 'C') ['IcC', 'IC']\n",
      "('e', 'C') ['ecC', 'eC']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
