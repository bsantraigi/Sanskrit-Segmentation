{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utilities import *\n",
    "from DCS import *\n",
    "from sentences import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodDict = pickle.load(open('mergedGood_v3.p', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fList = list(goodDict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dcsO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-318c5c77ef9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSeeDCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdcsO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dcsO' is not defined"
     ]
    }
   ],
   "source": [
    "# SeeDCS(dcsO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKT ANALYZE\n",
      "---------------\n",
      "na cEva mUrKa DarmeRa kevalenEva Sakyate   \n",
      "Analyzing: na\n",
      "0 : na [0, 'na', 2] \n",
      "-------------------------\n",
      "Analyzing: cEva\n",
      "0 : ca [1, 'ca', 2] \n",
      "1 : eva [2, 'eva', 2] \n",
      "-------------------------\n",
      "Analyzing: mUrKa\n",
      "0 : mUrKa [3, 'mUrKa', 49] [4, 'mUrKa', 51] \n",
      "-------------------------\n",
      "Analyzing: DarmeRa\n",
      "0 : DarmeRa [5, 'Darma', 89] [6, 'Darma', 91] \n",
      "-------------------------\n",
      "Analyzing: kevalenEva\n",
      "0 : kevalena [7, 'kevala', 89] [8, 'kevala', 91] \n",
      "0 : kevala [9, 'kevala', 3] \n",
      "0 : kevalA [10, 'kevala', 30] \n",
      "0 : kevale [11, 'kevala', 35] [12, 'kevala', 36] [13, 'kevala', 39] [14, 'kevala', 75] [15, 'kevala', 76] [16, 'kevala', 49] [17, 'kevala', 51] \n",
      "0 : ke [18, 'ka', 35] [19, 'ka', 75] [20, 'ka', 76] [21, 'ka', 36] [22, 'ka', 39] \n",
      "2 : valena [23, 'vala', 89] \n",
      "2 : vala [24, 'vala', 3] \n",
      "2 : vale [25, 'vala', 49] \n",
      "2 : vala [26, 'val', -32] \n",
      "2 : vale [27, 'val', -11] \n",
      "5 : inA [28, 'na', 30] \n",
      "6 : nEva [29, 'nava', 2] \n",
      "6 : na [30, 'na', 2] \n",
      "7 : eva [31, 'eva', 2] \n",
      "-------------------------\n",
      "Analyzing: Sakyate\n",
      "0 : Sakyate [32, 'SakyatA', 35] [33, 'SakyatA', 75] \n",
      "0 : Sakyate [34, 'Sak', -243] \n",
      "0 : Sakyate [35, 'Sak', 109] [36, 'Sak', -10] [37, 'Sak', 111] \n",
      "0 : Sakyate [38, 'SakyatA', 50] [39, 'SakyatA', 55] \n",
      "0 : Sakya [40, 'Sak', -32] \n",
      "5 : te [41, 'tad', 35] [42, 'tad', 75] [43, 'tad', 76] [44, 'tad', 36] [45, 'tad', 39] [46, 'tvad', 152] [47, 'tvad', 112] \n",
      "-------------------------\n",
      "Analyzing  na\n",
      "0 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "Analyzing  cEva\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "1 :  Eva [''] [{'verb': ['impft. [2] ac. du. 1']}]\n",
      "1 :  eva ['eva'] [{'indeclinable': ['prep.']}]\n",
      "Analyzing  mUrKa\n",
      "0 :  mUrKa ['mUrKa'] [{'noun': ['voc. sg. m.', 'voc. sg. n.']}]\n",
      "Analyzing  DarmeRa\n",
      "0 :  DarmeRa ['Darma'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "Analyzing  kevalenEva\n",
      "0 :  kevalena ['kevala'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "0 :  kevala ['kevala'] [{'compound': ['iic.']}]\n",
      "0 :  kevalA ['kevala'] [{'noun': ['nom. sg. f.']}]\n",
      "0 :  kevale ['kevala'] [{'noun': ['nom. pl. m.', 'loc. sg. m.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'acc. du. f.', 'nom. du. f.']}]\n",
      "0 :  ke ['ka'] [{'noun': ['nom. pl. m.', 'acc. du. f.', 'nom. du. f.', 'acc. du. n.', 'nom. du. n.']}]\n",
      "2 :  valena ['vala'] [{'noun': ['i. sg. m.']}]\n",
      "2 :  vala ['vala'] [{'compound': ['iic.']}]\n",
      "2 :  vale ['vala'] [{'noun': ['loc. sg. m.']}]\n",
      "2 :  vala ['val'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "2 :  vale ['val'] [{'verb': ['pr. [1] md. sg. 1']}]\n",
      "5 :  inA ['na'] [{'noun': ['nom. sg. f.']}]\n",
      "6 :  nEva ['nava'] [{'undetermined': ['adv.']}]\n",
      "6 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "7 :  Eva [''] [{'verb': ['impft. [2] ac. du. 1']}]\n",
      "7 :  eva ['eva'] [{'indeclinable': ['prep.']}]\n",
      "Analyzing  Sakyate\n",
      "0 :  Sakyate ['SakyatA'] [{'noun': ['acc. du. f.', 'nom. du. f.']}]\n",
      "0 :  Sakyate ['Sak'] [{'verb': ['pr. ps. sg. 3']}]\n",
      "0 :  Sakyate ['Sak'] [{'verbform': ['\"SKTMW255.html#H_zak\"'], 'noun': ['dat. sg. n.', 'dat. sg. m.'], 'verb': [['ppr. [4] ac.']]}]\n",
      "0 :  Sakyate ['SakyatA'] [{'noun': ['voc. du. f.', 'voc. sg. f.']}]\n",
      "0 :  Sakya ['Sak'] [{'verb': ['imp. [4] ac. sg. 2']}]\n",
      "5 :  te ['tad', 'tvad'] [{'noun': ['acc. du. n.', 'nom. du. n.', 'nom. pl. m.', 'acc. du. f.', 'nom. du. f.']}, {'noun': ['g. sg. *', 'dat. sg. *']}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# skt, dcs  = loadSentence(fList[7], goodDict[fList[7]])\n",
    "# SeeSentence(skt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fList = list(goodDict.keys())\n",
    "\n",
    "def ValidateSandhi(fi):\n",
    "    \n",
    "    f = fList[fi]\n",
    "    skt, dcs  = loadSentence(f, goodDict[f])\n",
    "    \n",
    "    if skt==None:\n",
    "        return\n",
    "#     print()\n",
    "#     print('=='*20)\n",
    "#     print(f.upper())\n",
    "#     SeeSentence(skt)\n",
    "    (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain) = SentencePreprocess(skt)\n",
    "\n",
    "    toSearch = []\n",
    "\n",
    "    for i in range(len(dcs.lemmas)):\n",
    "        lls = dcs.lemmas[i]\n",
    "        for j in range(len(lls)):\n",
    "            toSearch.append((i, rom_slp(lls[j]), int(dcs.cng[i][j])))\n",
    "\n",
    "    deactTuple = [False]*len(tuplesMain)\n",
    "    for qtup in toSearch:\n",
    "#         print('[QUERY]', qtup)\n",
    "        qcid = qtup[0]\n",
    "        qlem = qtup[1]\n",
    "        qcng = qtup[2]\n",
    "        activeChunk = chunkDict[qcid]\n",
    "        matchFound = False\n",
    "\n",
    "        for pos in activeChunk.keys():\n",
    "            for i in activeChunk[pos]:\n",
    "                if not deactTuple[i]:\n",
    "                    for tup in tuplesMain[i]:\n",
    "        #                 print(tup)\n",
    "                        if (tup[2] == qtup[1]) and (tup[3] == qtup[2]):\n",
    "#                             print('Pair Match:', tup)\n",
    "                            matchFound = True\n",
    "                            deactTuple[i] = True\n",
    "                            srch = (pos, i)\n",
    "                            break\n",
    "                if(matchFound):\n",
    "                    break\n",
    "            if(matchFound):\n",
    "                break\n",
    "\n",
    "        if not matchFound:\n",
    "            for pos in activeChunk.keys():\n",
    "                for i in activeChunk[pos]:\n",
    "                    if not deactTuple[i]:\n",
    "                        for tup in tuplesMain[i]:\n",
    "                            if tup[2] == qtup[1]:\n",
    "#                                 print('Lemma Match:', tup)\n",
    "                                matchFound = True\n",
    "                                deactTuple[i] = True\n",
    "                                srch = (pos, i)\n",
    "                                break\n",
    "                    if(matchFound):\n",
    "                        break\n",
    "                if(matchFound):\n",
    "                    break\n",
    "\n",
    "        if matchFound:\n",
    "#             print(srch)\n",
    "            n1 = tuplesMain[srch[1]][0][1]\n",
    "#             print(n1)\n",
    "            p1 = srch[0]\n",
    "            for pos in activeChunk.keys():\n",
    "                if(pos == srch[0]):\n",
    "                    for i in activeChunk[pos]:\n",
    "                        # Remove all\n",
    "                        deactTuple[i] = True\n",
    "                else:\n",
    "                    if(pos < p1):\n",
    "                        for i in activeChunk[pos]:\n",
    "                            # Deactivate Tuple\n",
    "                            if not deactTuple[i]:\n",
    "                                n2 = tuplesMain[i][0][2]\n",
    "                                if not CanCoExist_sandhi(pos, p1, n2, n1):\n",
    "                                    deactTuple[i] = True\n",
    "#                                     print('[REMOVED]:', tuplesMain[i])\n",
    "                    else:\n",
    "                        for i in activeChunk[pos]:\n",
    "                            # Deactivate Tuple\n",
    "                            if not deactTuple[i]:\n",
    "                                n2 = tuplesMain[i][0][2]\n",
    "                                if not CanCoExist_sandhi(p1, pos, n1, n2):\n",
    "                                    deactTuple[i] = True\n",
    "#                                     print('[REMOVED]:', tuplesMain[i])\n",
    "\n",
    "    print('Words remaining in skt[%s]:' % f, len(tuplesMain) - sum(deactTuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ValidateSandhi(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load ../TextSegmentation/Pickle_Files/350925.p\n",
      "Words remaining in skt[393550.p]: 0\n",
      "Words remaining in skt[174522.p]: 0\n",
      "Words remaining in skt[413337.p]: 8\n",
      "Words remaining in skt[249620.p]: 0\n",
      "Words remaining in skt[380152.p]: 0\n",
      "Words remaining in skt[271247.p]: 2\n",
      "Words remaining in skt[406065.p]: 0\n",
      "Words remaining in skt[147028.p]: 0\n",
      "Words remaining in skt[249219.p]: 0\n",
      "Words remaining in skt[124251.p]: 2\n",
      "Words remaining in skt[217409.p]: 1\n",
      "Words remaining in skt[321715.p]: 2\n",
      "Words remaining in skt[353362.p]: 4\n",
      "Words remaining in skt[215329.p]: 3\n",
      "Words remaining in skt[394679.p]: 0\n",
      "Words remaining in skt[327565.p]: 0\n",
      "Words remaining in skt[434288.p]: 2\n",
      "Words remaining in skt[390635.p]: 0\n",
      "Words remaining in skt[333111.p]: 15\n",
      "Words remaining in skt[254504.p]: 1\n",
      "Words remaining in skt[114281.p]: 0\n",
      "Words remaining in skt[353733.p]: 3\n",
      "Words remaining in skt[121703.p]: 1\n",
      "Words remaining in skt[6760.p]: 9\n",
      "Words remaining in skt[201334.p]: 1\n",
      "Words remaining in skt[201261.p]: 0\n",
      "Words remaining in skt[270650.p]: 0\n",
      "Words remaining in skt[257207.p]: 1\n",
      "Words remaining in skt[736.p]: 15\n",
      "Words remaining in skt[120465.p]: 0\n",
      "Words remaining in skt[158439.p]: 7\n",
      "Words remaining in skt[118168.p]: 0\n",
      "Words remaining in skt[154886.p]: 0\n",
      "Words remaining in skt[317722.p]: 1\n",
      "Words remaining in skt[278511.p]: 5\n",
      "Words remaining in skt[223080.p]: 3\n",
      "Words remaining in skt[310440.p]: 0\n",
      "Words remaining in skt[416849.p]: 0\n",
      "Words remaining in skt[403132.p]: 0\n",
      "Words remaining in skt[151723.p]: 0\n",
      "Words remaining in skt[409366.p]: 0\n",
      "Words remaining in skt[250135.p]: 6\n",
      "Words remaining in skt[286472.p]: 0\n",
      "Words remaining in skt[350664.p]: 0\n",
      "Words remaining in skt[226565.p]: 0\n",
      "Words remaining in skt[405028.p]: 3\n",
      "Words remaining in skt[147702.p]: 0\n",
      "Words remaining in skt[275950.p]: 0\n",
      "Words remaining in skt[412137.p]: 5\n",
      "Words remaining in skt[143751.p]: 0\n",
      "Words remaining in skt[316695.p]: 0\n",
      "Words remaining in skt[388052.p]: 5\n",
      "Words remaining in skt[293457.p]: 0\n",
      "Words remaining in skt[263745.p]: 0\n",
      "Words remaining in skt[354359.p]: 0\n",
      "Words remaining in skt[310351.p]: 1\n",
      "Words remaining in skt[293402.p]: 0\n",
      "Words remaining in skt[297149.p]: 3\n",
      "Words remaining in skt[63478.p]: 2\n",
      "Words remaining in skt[12765.p]: 0\n",
      "Words remaining in skt[291423.p]: 0\n",
      "Words remaining in skt[217052.p]: 0\n",
      "Words remaining in skt[298534.p]: 14\n",
      "Words remaining in skt[352526.p]: 1\n",
      "Words remaining in skt[193442.p]: 6\n",
      "Words remaining in skt[334302.p]: 0\n",
      "Words remaining in skt[157844.p]: 2\n",
      "Words remaining in skt[237913.p]: 0\n",
      "Words remaining in skt[416620.p]: 0\n",
      "Words remaining in skt[396107.p]: 0\n",
      "Words remaining in skt[381599.p]: 0\n",
      "Words remaining in skt[432971.p]: 1\n",
      "Words remaining in skt[128252.p]: 0\n",
      "Words remaining in skt[313433.p]: 3\n",
      "Words remaining in skt[131352.p]: 0\n",
      "Words remaining in skt[293490.p]: 0\n",
      "Words remaining in skt[119475.p]: 0\n",
      "Words remaining in skt[10767.p]: 5\n",
      "Words remaining in skt[265420.p]: 1\n",
      "Words remaining in skt[131033.p]: 0\n",
      "Words remaining in skt[383972.p]: 1\n",
      "Words remaining in skt[307264.p]: 0\n",
      "Words remaining in skt[23353.p]: 0\n",
      "Words remaining in skt[145526.p]: 0\n",
      "Words remaining in skt[159593.p]: 5\n",
      "Words remaining in skt[270395.p]: 3\n",
      "Words remaining in skt[185675.p]: 0\n",
      "Words remaining in skt[263404.p]: 4\n",
      "Words remaining in skt[356919.p]: 0\n",
      "Words remaining in skt[14755.p]: 0\n",
      "Words remaining in skt[70344.p]: 0\n",
      "Words remaining in skt[262308.p]: 5\n",
      "Words remaining in skt[397651.p]: 0\n",
      "Words remaining in skt[419687.p]: 1\n",
      "Words remaining in skt[326123.p]: 0\n",
      "Words remaining in skt[425359.p]: 0\n",
      "Words remaining in skt[280154.p]: 2\n",
      "Words remaining in skt[287508.p]: 6\n",
      "Words remaining in skt[261576.p]: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    ValidateSandhi(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
