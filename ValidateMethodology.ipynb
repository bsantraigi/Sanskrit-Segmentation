{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utilities import *\n",
    "from DCS import *\n",
    "from sentences import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# goodDict = pickle.load(open('mergedGood_v3.p', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fList = list(goodDict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SeeDCS(dcsO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# skt, dcs  = loadSentence(fList[7], goodDict[fList[7]])\n",
    "# SeeSentence(skt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56974"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pf1 = pd.read_csv('extras/pvbnhi@pf.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "df_pf2 = pd.read_csv('extras/pvbnhi@skt.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "df_pf3 = pd.read_csv('extras/pvbnhi@upd.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "\n",
    "# df_pf1['miss'] = df_pf1[df_pf1['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "df_pf1 = df_pf1[df_pf1['miss'] == '0']\n",
    "df_pf1['folder'] = '../TextSegmentation/Pickle_Files/'\n",
    "\n",
    "# df_pf2['miss'] = df_pf2[df_pf2['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "df_pf2 = df_pf2[df_pf2['miss'] == '0']\n",
    "df_pf2['folder'] = '../TextSegmentation/corrected_10to20/'\n",
    "\n",
    "# df_pf3['miss'] = df_pf3[df_pf3['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "df_pf3 = df_pf3[df_pf3['miss'] == '0']\n",
    "df_pf3['folder'] = '../TextSegmentation/Updated Pickles/'\n",
    "\n",
    "frames = [df_pf1, df_pf2, df_pf3]\n",
    "df_pf = pd.concat(frames)\n",
    "%reset_selective df_pf1\n",
    "%reset_selective df_pf2\n",
    "%reset_selective df_pf3\n",
    "\n",
    "goodFileDict = {}\n",
    "for index, row in df_pf.iterrows():\n",
    "    goodFileDict['%d.p' % row['file']] = '%s%d.p' % (row['folder'], row['file'])\n",
    "len(goodFileDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fList = list(goodFileDict.keys())\n",
    "\n",
    "def ValidateSandhi(fi = -1, fName = '', fPath='', verbose=False):\n",
    "    \n",
    "    if(fi >= 0):\n",
    "        f = fList[fi]\n",
    "        fPath = goodFileDict[f]\n",
    "        skt, dcs  = loadSentence(f, fPath)\n",
    "    else:\n",
    "        f = fName\n",
    "        skt, dcs  = loadSentence(fName, fPath)\n",
    "    if skt==None:\n",
    "        return\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('=='*20)\n",
    "        print(f.upper())\n",
    "        print(fPath)\n",
    "#     SeeSentence(skt)\n",
    "    (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain) = SentencePreprocess(skt)\n",
    "    if(len(chunkDict) != len(dcs.lemmas)):\n",
    "        print('*'*66)\n",
    "        print('REPORTING LENGTH MISMATCH:', fPath)\n",
    "        print('*'*66)\n",
    "        return 0\n",
    "    toSearch = []\n",
    "\n",
    "    for i in range(len(dcs.lemmas)):\n",
    "        lls = dcs.lemmas[i]\n",
    "        for j in range(len(lls)):\n",
    "            # (chunk, lemma, cng)\n",
    "            toSearch.append((i, rom_slp(lls[j]), int(dcs.cng[i][j])))\n",
    "\n",
    "    deactTuple = [False]*len(tuplesMain)\n",
    "    for qtup in toSearch:\n",
    "        if verbose:\n",
    "            print('\\n\\n[QUERY]', qtup)\n",
    "        qcid = qtup[0]\n",
    "        qlem = qtup[1]\n",
    "        qcng = qtup[2]\n",
    "        activeChunk = chunkDict[qcid]\n",
    "        matchFound = False\n",
    "\n",
    "        for pos in activeChunk.keys():\n",
    "            for i in activeChunk[pos]:\n",
    "                if not deactTuple[i]:\n",
    "                    for tup in tuplesMain[i]:\n",
    "        #                 print(tup)\n",
    "                        if (tup[2] == qtup[1]) and (tup[3] == qtup[2]):\n",
    "                            if(verbose):\n",
    "                                print('[PAIR Match] chunk_%d, pos_%d, [%s], cng(%d)' % (qcid, pos, tup[2], tup[3]))\n",
    "                            matchFound = True\n",
    "                            deactTuple[i] = True\n",
    "                            srch = (pos, i)\n",
    "                            break\n",
    "                if(matchFound):\n",
    "                    break\n",
    "            if(matchFound):\n",
    "                break\n",
    "\n",
    "        if not matchFound:\n",
    "            for pos in activeChunk.keys():\n",
    "                for i in activeChunk[pos]:\n",
    "                    if not deactTuple[i]:\n",
    "                        for tup in tuplesMain[i]:\n",
    "                            if tup[2] == qtup[1]:\n",
    "                                if(verbose):\n",
    "                                    print('[LEMMA Match] chunk_%d, pos_%d, [%s], cng(%d)' % (qcid, pos, tup[2], tup[3]))\n",
    "                                matchFound = True\n",
    "                                deactTuple[i] = True\n",
    "                                srch = (pos, i)\n",
    "                                break\n",
    "                    if(matchFound):\n",
    "                        break\n",
    "                if(matchFound):\n",
    "                    break\n",
    "\n",
    "        if matchFound:\n",
    "            if verbose:\n",
    "                print('[REMOVED] (Pos, Lemma, Names)')\n",
    "#             print(srch)\n",
    "            n1 = tuplesMain[srch[1]][0][1]\n",
    "#             print(n1)\n",
    "            p1 = srch[0]\n",
    "            for pos in activeChunk.keys():\n",
    "                if(pos == srch[0]):\n",
    "                    for i in activeChunk[pos]:\n",
    "                        # Remove all\n",
    "                        deactTuple[i] = True\n",
    "                        if verbose:\n",
    "                            for t_rem in tuplesMain[i]:\n",
    "                                print('(pos_%d, [%s], [%s]) ' % (pos, t_rem[2], t_rem[1]))\n",
    "                else:\n",
    "                    if(pos < p1):\n",
    "                        for i in activeChunk[pos]:\n",
    "                            # Deactivate Tuple\n",
    "                            if not deactTuple[i]:\n",
    "                                n2 = tuplesMain[i][0][2]\n",
    "                                if not CanCoExist_sandhi(pos, p1, n2, n1):\n",
    "                                    deactTuple[i] = True\n",
    "                                    if(verbose):\n",
    "                                        for t_rem in tuplesMain[i]:\n",
    "                                            print('(pos_%d, [%s], [%s]) ' % (pos, t_rem[2], t_rem[1]))\n",
    "                    else:\n",
    "                        for i in activeChunk[pos]:\n",
    "                            # Deactivate Tuple\n",
    "                            if not deactTuple[i]:\n",
    "                                n2 = tuplesMain[i][0][2]\n",
    "                                if not CanCoExist_sandhi(p1, pos, n1, n2):\n",
    "                                    deactTuple[i] = True\n",
    "                                    if(verbose):\n",
    "                                        for t_rem in tuplesMain[i]:\n",
    "                                            print('(pos_%d, [%s], [%s]) ' % (pos, t_rem[2], t_rem[1]))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('___[NOT FOUND]___')\n",
    "    if verbose:\n",
    "        print('\\n[REMAINING] (cid, pos, tuples)')\n",
    "        for cid in list(chunkDict.keys()):\n",
    "            for pos in chunkDict[cid].keys():\n",
    "                for tid in chunkDict[cid][pos]:\n",
    "                    if not deactTuple[tid]:\n",
    "                        print('\\n', [cid, pos], '\\t', end='')\n",
    "                        for tup in tuplesMain[tid]:\n",
    "                            print(tup[2], end=', ')\n",
    "    if verbose:\n",
    "        print('\\n\\nWords remaining in skt[%s]:' % f, len(tuplesMain) - sum(deactTuple))\n",
    "    return len(tuplesMain) - sum(deactTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "8494.P\n",
      "../TextSegmentation/Updated Pickles/8494.p\n",
      "\n",
      "\n",
      "[QUERY] (0, 'DAnya', 1)\n",
      "[LEMMA Match] chunk_0, pos_0, [DAnya], cng(3)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_0, [DAnya], [DAnya]) \n",
      "(pos_0, [DAna], [DAnI]) \n",
      "(pos_0, [DAna], [DAnI]) \n",
      "(pos_4, [f], [ara]) \n",
      "(pos_4, [f], [ara]) \n",
      "(pos_4, [f], [ara]) \n",
      "\n",
      "\n",
      "[QUERY] (0, 'rASi', 1)\n",
      "[LEMMA Match] chunk_0, pos_5, [rASi], cng(49)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_4, [ara], [ara]) \n",
      "(pos_4, [ara], [arA]) \n",
      "(pos_5, [rASi], [rASO]) \n",
      "(pos_5, [rA], [rA]) \n",
      "(pos_5, [rA], [rA]) \n",
      "(pos_6, [ASa], [ASO]) \n",
      "(pos_6, [ASa], [ASO]) \n",
      "(pos_6, [ASu], [ASO]) \n",
      "(pos_6, [ASu], [ASO]) \n",
      "(pos_6, [ASa], [ASO]) \n",
      "\n",
      "\n",
      "[QUERY] (1, 'sTA', -190)\n",
      "[PAIR Match] chunk_1, pos_0, [sTA], cng(-190)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_0, [sTta], [sTitam]) \n",
      "(pos_0, [sTta], [sTitam]) \n",
      "(pos_0, [sTta], [sTitam]) \n",
      "(pos_0, [sTA], [sTitam]) \n",
      "\n",
      "\n",
      "[QUERY] (2, 'mAsa', 1)\n",
      "[LEMMA Match] chunk_2, pos_0, [mAsa], cng(31)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_0, [mAsa], [mAsam]) \n",
      "(pos_0, [mAsa], [mAsam]) \n",
      "(pos_0, [mAsa], [mAsam]) \n",
      "(pos_0, [mA], [mA]) \n",
      "(pos_0, [mad], [mA]) \n",
      "(pos_1, [as], [Asam]) \n",
      "(pos_1, [as], [Asam]) \n",
      "\n",
      "\n",
      "[QUERY] (3, 'tatas', 1)\n",
      "[LEMMA Match] chunk_3, pos_0, [tatas], cng(2)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_0, [tatas], [tatas]) \n",
      "(pos_0, [tata], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tad], [tatas]) \n",
      "(pos_0, [tan], [tatas]) \n",
      "\n",
      "\n",
      "[QUERY] (4, 'nizka', 1)\n",
      "[PAIR Match] chunk_4, pos_0, [nizka], cng(1)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_0, [nizka], [nizka]) \n",
      "\n",
      "\n",
      "[QUERY] (4, 'traya', 1)\n",
      "[PAIR Match] chunk_4, pos_5, [traya], cng(1)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_5, [traya], [traya]) \n",
      "\n",
      "\n",
      "[QUERY] (5, 'sama', 1)\n",
      "[LEMMA Match] chunk_5, pos_0, [sama], cng(31)\n",
      "[REMOVED] (Pos, Lemma, Names)\n",
      "(pos_0, [sama], [samam]) \n",
      "(pos_0, [sama], [samam]) \n",
      "(pos_0, [sama], [samam]) \n",
      "\n",
      "[REMAINING] (cid, pos, tuples)\n",
      "\n",
      " [0, 4] \ta, \n",
      "Words remaining in skt[8494.p]: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ValidateSandhi(fName = '305755.p', fPath= '../TextSegmentation/Pickle_Files/305755.p', verbose=True)\n",
    "ValidateSandhi(2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "got = 0\n",
    "for i in range(500):\n",
    "    r = ValidateSandhi(i, verbose= False)\n",
    "#     print('Word Remaining:', r)\n",
    "    if r > 0:\n",
    "        got += 1\n",
    "        ValidateSandhi(i, verbose=True)\n",
    "    if got >= 100:\n",
    "        break\n",
    "print('-'*50, '\\nGot', got, 'problems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(goodFileDict) ==dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8454",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-95d9a3f2966f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgoodFileDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8454\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 8454"
     ]
    }
   ],
   "source": [
    "goodFileDict[8454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
