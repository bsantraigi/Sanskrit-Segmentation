{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with:  44790  Files\n",
      "\n",
      "Word2Vec(vocab=66936, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "from DCS import DCS\n",
    "from sentences import word_new, chunks, sentences\n",
    "from utilities import printProgress, validatePickleName, pickleFixLoad\n",
    "import re\n",
    "from romtoslp import rom_slp\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if(sys.version_info < (3, 0)):\n",
    "        warnings.warn(\"\\nPython version 3 or greater is required. Python 2.x is not tested.\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "       Folder @ sentencesPath contains pickle files for \"sentences\" object\n",
    "       Folder @ path2 contains pickle files for the same sentences\n",
    "       as in Folder @ sentencesPath but its DCS equivalent\n",
    "    \"\"\"\n",
    "    sentencesPath ='../TextSegmentation/Pickles/'\n",
    "    dcsPath = '../Text Segmentation/DCS_pick/'\n",
    "\n",
    "    sentenceFiles=set(sorted(os.listdir(sentencesPath)))\n",
    "    dcsFiles=set(sorted(os.listdir(dcsPath)))\n",
    "\n",
    "    \"\"\"\n",
    "    Get common dcs and sentences files\n",
    "    \"\"\"\n",
    "    print()\n",
    "    minSize = min(len(sentenceFiles), len(dcsFiles))\n",
    "    commonFiles = []\n",
    "    \n",
    "    for sPickle in sentenceFiles:        \n",
    "        if sPickle in dcsFiles:\n",
    "            sPickle = validatePickleName(sPickle)\n",
    "            if sPickle != \"\":                \n",
    "                commonFiles.append(sPickle)\n",
    "\n",
    "    commonFiles = list(set(commonFiles))\n",
    "\n",
    "    print(\"Testing with: \",len(commonFiles), \" Files\")\n",
    "\n",
    "    \"\"\"\n",
    "    Load the CBOW pickle\n",
    "    \"\"\"\n",
    "    print()\n",
    "    model_cbow = pickleFixLoad('extras/modelpickle10.p')\n",
    "    print(model_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeRowStochastic(matrix):\n",
    "    rowCount = matrix.shape[0]\n",
    "    for row in range(rowCount):\n",
    "        s = np.sum(matrix[row, :])\n",
    "        if(s!=0):\n",
    "            matrix[row, :] = matrix[row, :]/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RWR(prioriVec, transMat, restartP, maxIteration, queryList):\n",
    "    \"\"\"\n",
    "    Run Random walk with restart\n",
    "    until \n",
    "    we reach steady state or max iteration steps\n",
    "    \"\"\"\n",
    "    \n",
    "#     MERGE THE NEW QUERY NODE(IF ANY), CHANGES IN TRANSMAT AND PRIORI-VEC\n",
    "    \n",
    "    if(len(queryList) > 1):\n",
    "        dest = queryList[0]\n",
    "        \n",
    "        # Using the max probability logic\n",
    "        transMat[dest, :] = np.max(transMat[queryList, :], axis=0)\n",
    "        transMat[queryList[1:], :] = 0   \n",
    "        \n",
    "        transMat[:, dest] = np.max(transMat[:, queryList], axis=1)\n",
    "        transMat[:, queryList[1:]] = 0   \n",
    "#     print(transMat)\n",
    "#         TODO - Using the sum probability logic\n",
    "    \n",
    "    eps = 0.0000000000001    # the error difference, which should ideally be zero but can never be attained.\n",
    "    \n",
    "    n = prioriVec.shape[1]\n",
    "    papMat = np.array(prioriVec)\n",
    "    \n",
    "    rVec = np.zeros((1, n))    \n",
    "#     print(n)\n",
    "    for i in queryList:\n",
    "#         print(i)\n",
    "        rVec[0, i] = 1/len(queryList)\n",
    "    \n",
    "    for i in range(maxIteration):        \n",
    "#        print('shapes',papMat.shape,va.shape,prevMat.shape)\n",
    "        newMat = (1 - restartP) * np.dot(papMat, transMat) + restartP * np.mat(rVec)\n",
    "        diff = np.absolute(papMat - newMat)\n",
    "        diffMax = np.argmax(diff)\n",
    "        papMat = newMat\n",
    "        if  abs(diffMax) < eps and maxIteration/10 > 10:\n",
    "            break\n",
    "                  \n",
    "    return(papMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------SENTENCE--------------\n",
      "yaTA ca varDate muktA taTA mOlyaM ca varDate   \n",
      "\n",
      "Analyzing  yathā\n",
      "0 :  yaTÄ ['yathÄ\\x81'] [{'indeclinable': ['conj.']}]\n",
      "\n",
      "Analyzing  ca\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "\n",
      "Analyzing  vardhate\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'verb': ['pr. [1] md. sg. 3']}]\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'verb': ['pr. [1] md. sg. 3']}]\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'noun': ['dat. sg. n.', 'dat. sg. m.'], 'verb': [['ppr. [1] ac.']], 'verbform': ['\"SKTMW248.html#H_v.rdh#1\"']}]\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'noun': ['dat. sg. n.', 'dat. sg. m.'], 'verb': [['ppr. [1] ac.']], 'verbform': ['\"SKTMW248.html#H_v.rdh#1\"']}]\n",
      "0 :  varDa ['vá¹\\x9bdh_1'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "0 :  varDa ['vá¹\\x9bdh_1'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "5 :  te ['tad', 'yuá¹£mad'] [{'noun': ['acc. du. n.', 'nom. du. n.', 'nom. pl. m.', 'acc. du. f.', 'nom. du. f.']}, {'noun': ['g. sg. *', 'dat. sg. *']}]\n",
      "5 :  te ['tad', 'yuá¹£mad'] [{'noun': ['acc. du. n.', 'nom. du. n.', 'nom. pl. m.', 'acc. du. f.', 'nom. du. f.']}, {'noun': ['g. sg. *', 'dat. sg. *']}]\n",
      "\n",
      "Analyzing  muktā\n",
      "0 :  muktÄ ['mukta', 'muc_1'] [{'noun': ['nom. sg. f.']}, {'verb': ['pp.']}]\n",
      "\n",
      "Analyzing  taTA\n",
      "\n",
      "Analyzing  mOlyam\n",
      "\n",
      "Analyzing  ca\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "0 :  ca ['ca'] [{'indeclinable': ['conj.']}]\n",
      "\n",
      "Analyzing  vardhate\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'verb': ['pr. [1] md. sg. 3']}]\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'verb': ['pr. [1] md. sg. 3']}]\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'noun': ['dat. sg. n.', 'dat. sg. m.'], 'verb': [['ppr. [1] ac.']], 'verbform': ['\"SKTMW248.html#H_v.rdh#1\"']}]\n",
      "0 :  varDate ['vá¹\\x9bdh_1'] [{'noun': ['dat. sg. n.', 'dat. sg. m.'], 'verb': [['ppr. [1] ac.']], 'verbform': ['\"SKTMW248.html#H_v.rdh#1\"']}]\n",
      "0 :  varDa ['vá¹\\x9bdh_1'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "0 :  varDa ['vá¹\\x9bdh_1'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "5 :  te ['tad', 'yuá¹£mad'] [{'noun': ['acc. du. n.', 'nom. du. n.', 'nom. pl. m.', 'acc. du. f.', 'nom. du. f.']}, {'noun': ['g. sg. *', 'dat. sg. *']}]\n",
      "5 :  te ['tad', 'yuá¹£mad'] [{'noun': ['acc. du. n.', 'nom. du. n.', 'nom. pl. m.', 'acc. du. f.', 'nom. du. f.']}, {'noun': ['g. sg. *', 'dat. sg. *']}]\n",
      "0 yaTÄ 11 mukta \n",
      "[0 8 8 0 0 0 0 0 0 8 8 8 8 8 0 0 0 0 0 0 8 8]\n",
      "------------DCS--------------\n",
      "yaTA ca varDate muktA taTA mOlyaM ca varDate   \n",
      "['yathÄ\\x81', 'ca', 'vá¹\\x9bdh', 'muktÄ\\x81', 'tathÄ\\x81', 'maulya', 'ca', 'vá¹\\x9bdh']\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"-----------------------------------------------------------\n",
    "    PART 2\n",
    "        1. LOAD A SENTENCE\n",
    "        2. UNIFORM PRIOR PROB.\n",
    "        3. SET QUERY NODE\n",
    "        3. RUN RANDOM WALK\n",
    "        4. CHOOSE WINNER\n",
    "        5. MERGE QUERY NODES\n",
    "        6. RERUN FROM 2\n",
    "    -----------------------------------------------------------\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Test with a sentence\n",
    "    \"\"\"\n",
    "\n",
    "#     print(commonFiles[10])\n",
    "#     fName = commonFiles[460]\n",
    "    fName = \"79.p\"\n",
    "    sentenceObj = pickleFixLoad(sentencesPath + fName)\n",
    "    dcsObj = pickleFixLoad(dcsPath + fName)\n",
    "    \n",
    "    print()\n",
    "    print(\"------------SENTENCE--------------\")\n",
    "    print(sentenceObj.sentence)\n",
    "    \n",
    "    \"\"\"\n",
    "    Considering word names only\n",
    "    ***{Word forms can also be used}\n",
    "    \"\"\"\n",
    "    chunkDict = {}\n",
    "    wordList = []\n",
    "    revMap2Chunk = []\n",
    "    qu = []\n",
    "    \n",
    "    cid = -1\n",
    "    for chunk in sentenceObj.chunk:\n",
    "        print()\n",
    "        cid = cid+1\n",
    "        chunkDict[cid] = {}\n",
    "        canBeQuery = 0\n",
    "        if len(chunk.chunk_words.keys()) == 1:\n",
    "            canBeQuery = 1\n",
    "        print(\"Analyzing \", chunk.chunk_name)\n",
    "        for pos in chunk.chunk_words.keys():\n",
    "            chunkDict[cid][pos] = []\n",
    "            if(canBeQuery == 1) and (len(chunk.chunk_words[pos]) == 1):\n",
    "                canBeQuery = 2                \n",
    "            for word_sense in chunk.chunk_words[pos]:\n",
    "                if(len(word_sense.lemmas) > 0):\n",
    "                    wordList.append(rom_slp(word_sense.lemmas[0]))\n",
    "                    k = len(wordList) - 1\n",
    "                    chunkDict[cid][pos].append(k)\n",
    "                    revMap2Chunk.append((cid, pos))\n",
    "                    print(pos, \": \", rom_slp(word_sense.names), word_sense.lemmas, word_sense.forms)                    \n",
    "                    if canBeQuery == 2:\n",
    "                        qu.append(k)\n",
    "    \n",
    "    \n",
    "    nodeCount = len(wordList)    \n",
    "    TransitionMat = np.zeros((nodeCount, nodeCount))\n",
    "    \n",
    "    \"\"\"\n",
    "    FIXME:\n",
    "    1. HOW TO DO SMOOTHING?\n",
    "    2. HOW TO CONVERT WORD2VEC SIM. TO PROB.\n",
    "    \"\"\"\n",
    "    \n",
    "    for row in range(nodeCount):\n",
    "        for col in range(nodeCount):\n",
    "            if row != col:\n",
    "                try:\n",
    "                    TransitionMat[row][col] = model_cbow.similarity(wordList[row], wordList[col])\n",
    "                except KeyError:\n",
    "                    TransitionMat[row][col] = -1 #WHAT TO DO HERE??\n",
    "            else:\n",
    "                TransitionMat[row][col] = -1 #WHAT TO DO HERE??\n",
    "    \n",
    "    \n",
    "    TransitionMat = (TransitionMat + 1)/2\n",
    "    MakeRowStochastic(TransitionMat)\n",
    "    for q in qu:\n",
    "        print(q,wordList[q], end=\" \")\n",
    "\n",
    "    print()\n",
    "        \n",
    "    print(sum(TransitionMat!=0))\n",
    "#     print(np.max(TransitionMat), np.min(TransitionMat))\n",
    "        \n",
    "    \n",
    "    print(\"------------DCS--------------\")\n",
    "    print(dcsObj.sentence)\n",
    "    solution = [w for w in dcsObj.dcs_chunks]\n",
    "    print(solution)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INITIALIZATION OF RWR VECTORS/MATRICES\n",
    "deactivated = []\n",
    "prioriVec = np.ones((1, nodeCount))\n",
    "for q in qu:\n",
    "    prioriVec[0, q] = 0\n",
    "prioriVec[0, qu[0]] = 1\n",
    "def deactivate(index):    \n",
    "    deactivated.append(index)\n",
    "#     print(\"Removed: \", wordList[index])\n",
    "    prioriVec[0,index] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yaTÄ\\x81', 'mukta', 'ca', 'ca', 'tad', 'tad']\n",
      "['yathÄ\\x81', 'ca', 'vá¹\\x9bdh', 'muktÄ\\x81', 'tathÄ\\x81', 'maulya', 'ca', 'vá¹\\x9bdh']\n",
      "\n",
      "Accuracy:  25.0\n"
     ]
    }
   ],
   "source": [
    "while(len(qu) > 0):\n",
    "#     print(\"-----------------------------------------------------------\")\n",
    "#     print(\"---------------------- NEW ROUND --------------------------\")\n",
    "#     print(\"-----------------------------------------------------------\")\n",
    "    try:\n",
    "        uniform_prob = 1/(nodeCount - len(qu) + 1 - len(deactivated))\n",
    "        prioriVec = (prioriVec != 0) * uniform_prob\n",
    "\n",
    "#         print(qu)\n",
    "#         print(deactivated)\n",
    "#         print(prioriVec)\n",
    "#         print(np.sum(prioriVec))\n",
    "        # print(TransitionMat)\n",
    "\n",
    "        rp = 0.4 # This is to be set based on graph diameter\n",
    "        weights = RWR(prioriVec, TransitionMat, rp, 100, qu)\n",
    "        ranking = np.asarray(weights.argsort()).reshape(-1)\n",
    "#         print(\"Weights: \",weights)\n",
    "#         print(\"Rankings: \",ranking)\n",
    "        cid = -1\n",
    "        pos = -1\n",
    "        # FIND OUT THE WINNER\n",
    "        for r in ranking[::-1]:\n",
    "            if(r in qu or r in deactivated):\n",
    "                continue\n",
    "#             print(r, wordList[r])\n",
    "            qu.append(r)\n",
    "            prioriVec[0,r] = 0\n",
    "            # Remove overlapping competitors\n",
    "            cid, pos = revMap2Chunk[r]\n",
    "#             print(cid, pos)\n",
    "            break\n",
    "\n",
    "        # Remove overlapping words\n",
    "        activeChunk = chunkDict[cid]\n",
    "        r = qu[len(qu) - 1]\n",
    "        for _pos in activeChunk:\n",
    "            if(_pos < pos):\n",
    "                for index in activeChunk[_pos]:\n",
    "                    if index not in deactivated and index not in qu:\n",
    "                        w = wordList[index]\n",
    "                        if(_pos+len(w)-1 > pos):\n",
    "                            deactivate(index)                    \n",
    "            elif(_pos > pos):\n",
    "                winwin = wordList[r]\n",
    "                for index in activeChunk[_pos]:\n",
    "                    if index not in deactivated and index not in qu:\n",
    "                        if(_pos < pos + len(winwin) - 1):\n",
    "                            deactivate(index)                    \n",
    "            else:\n",
    "                for index in activeChunk[_pos]:\n",
    "                    if(index != r):\n",
    "                        if index not in deactivated:\n",
    "                            deactivate(index)                    \n",
    "\n",
    "#         print(deactivated)\n",
    "    except KeyError:\n",
    "        break\n",
    "\n",
    "# a = [5, 13, 6, 1, 4, 10]\n",
    "print([wordList[i] for i in qu])\n",
    "#     print(wordList[i], end=\" \")\n",
    "# print()\n",
    "print(solution)\n",
    "# for i in qu:\n",
    "#     print(wordList[i] in solution, end=\" \")\n",
    "\n",
    "print()\n",
    "print(\"Accuracy: \", 100*sum(list(map(lambda x: wordList[x] in solution, qu)))/len(solution))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=66936, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
