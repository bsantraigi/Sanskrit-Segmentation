{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "from utilities import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "co_ocMat = pickle.load(open('extras/all_dcs_lemmas_matrix.p', 'rb'))\n",
    "cngMat = pickle.load(open('extras/same_cng_matrix.p', 'rb'))\n",
    "dcsLemmas = pickleFixLoad('extras/unique_dcs_lemmas.p')\n",
    "cng_ordered_list = pickle.load(open('extras/list_of_uniq_cngs.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# def AutoID():\n",
    "#     aid = -1\n",
    "#     while(True):\n",
    "#         aid += 1\n",
    "#         yield aid\n",
    "\n",
    "# auto_id = AutoID()\n",
    "\n",
    "# indexDict = defaultdict(lambda: next(auto_id))\n",
    "# temp = [indexDict[x] for x in dcsLemmas]\n",
    "# pickle.dump(dict(indexDict), open('dcsLemma2index.p', 'wb'))\n",
    "\n",
    "# indexDictCng = defaultdict(lambda: next(auto_id))\n",
    "# temp = [indexDictCng[x] for x in cng_ordered_list]\n",
    "# pickle.dump(dict(indexDictCng), open('cng2index_dict.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current folder contains:  44790  Files\n"
     ]
    }
   ],
   "source": [
    "from SktWsegRWR_utf8 import *\n",
    "from AlgoTestFactory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMultiProcess(prCount, upto, method, savePath):\n",
    "    print(\"Using\", prCount, \"Processes\")\n",
    "    filePerProcess = upto/prCount\n",
    "    testerProcesses = [None]*prCount\n",
    "    for thId in range(0,prCount):\n",
    "        testerProcesses[thId] = AlgoTestFactory(\n",
    "            [int(thId*filePerProcess), int((thId + 1)*filePerProcess)], \n",
    "            processID = thId, method=method, storeAccuracies=False, savePath = savePath)\n",
    "        testerProcesses[thId].start()\n",
    "\n",
    "    for p in testerProcesses:\n",
    "        p.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 Processes\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Using word2vec\n",
      "Process Finished (accuracies not saved to disk)\n",
      "42.857142857142854\n",
      "Process Finished (accuracies not saved to disk)\n",
      "0.0\n",
      "60.0\n",
      "Process Finished (accuracies not saved to disk)\n",
      "23.076923076923077\n",
      "Process Finished (accuracies not saved to disk)\n",
      "28.571428571428573\n",
      "50.0\n",
      "Process Finished (accuracies not saved to disk)\n",
      "Process Finished (accuracies not saved to disk)\n",
      "42.857142857142854\n",
      "0.0\n",
      "Process Finished (accuracies not saved to disk)\n",
      "28.571428571428573\n",
      "7.6923076923076925\n",
      "Process Finished (accuracies not saved to disk)\n",
      "Process Finished (accuracies not saved to disk)\n",
      "13.333333333333334\n",
      "Process Finished (accuracies not saved to disk)\n"
     ]
    }
   ],
   "source": [
    "runMultiProcess(10, 20, Method.word2vec, 'w2v_1_june')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 Processes\n",
      "Using type2type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process AlgoTestFactory-39:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bishal/Documents/summer project/TxSeg_Summer/AlgoTestFactory.py\", line 75, in run\n",
      "    result = self.algo.predict(sentenceObj, dcsObj)\n",
      "  File \"/home/bishal/Documents/summer project/TxSeg_Summer/SktWsegRWR_utf8.py\", line 162, in predict\n",
      "    TransitionMat = self.probModels.get_cng2cng_mat(cngList)\n",
      "  File \"/home/bishal/Documents/summer project/TxSeg_Summer/ProbModels.py\", line 29, in get_cng2cng_mat\n",
      "    cngIndexList = list(map(lambda x:cng2index_dict[str(x)], cngList))\n",
      "  File \"/home/bishal/Documents/summer project/TxSeg_Summer/ProbModels.py\", line 29, in <lambda>\n",
      "    cngIndexList = list(map(lambda x:cng2index_dict[str(x)], cngList))\n",
      "KeyError: 'none'\n"
     ]
    }
   ],
   "source": [
    "runMultiProcess(1, 2, Method.type2type, 't2t_1_june')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 Processes\n",
      "Using word2word\n",
      "Using word2word\n",
      "Using word2word\n",
      "Using word2word\n",
      "Using word2word\n",
      "Using word2word\n",
      "Using word2word\n",
      "60.0\n",
      "Process Finished (accuracies not saved to disk)\n",
      "Using word2word\n",
      "42.857142857142854\n",
      "Using word2word\n",
      "Process Finished (accuracies not saved to disk)\n",
      "Process Finished (accuracies not saved to disk)\n",
      "Using word2word0.0\n",
      "Process Finished (accuracies not saved to disk)\n",
      "\n",
      "14.285714285714286\n",
      "Process Finished (accuracies not saved to disk)\n",
      "50.0\n",
      "23.076923076923077\n",
      "Process Finished (accuracies not saved to disk)\n",
      "42.857142857142854\n",
      "7.6923076923076925\n",
      "Process Finished (accuracies not saved to disk)\n",
      "33.333333333333336\n",
      "28.571428571428573\n",
      "Process Finished (accuracies not saved to disk)\n",
      "Process Finished (accuracies not saved to disk)\n",
      "0.0\n",
      "Process Finished (accuracies not saved to disk)\n"
     ]
    }
   ],
   "source": [
    "runMultiProcess(10, 20, Method.word2word, 'w2w_1_june')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sentences import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentencePath = '../TextSegmentation/Pickles/'\n",
    "sentenceObj = pickleFixLoad(sentencePath + '14265.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anena mantreRa sahasrajaptena kavitvaM karoti nAtra saMdehaH    \n",
      "Analyzing  anena\n",
      "0 :  anena ['ayam'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "Analyzing  mantreRa\n",
      "0 :  mantreá¹a ['mantra'] [{'noun': ['i. sg. m.', 'i. sg. n.']}]\n",
      "Analyzing  sahasrajaptena\n",
      "0 :  sahasra ['sahasra'] [{'compound': ['iic.']}]\n",
      "11 :  ina ['na'] [{'noun': ['voc. sg. m.', 'voc. sg. n.']}]\n",
      "12 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "7 :  japtena ['jap'] [{'noun': ['i. sg. n.', 'i. sg. m.'], 'verbform': ['\"SKTMW93.html#H_jap\"'], 'verb': [['pp.']]}]\n",
      "7 :  japta ['jap'] [{'verbform': ['\"SKTMW93.html#H_jap\"'], 'compound': ['iic.'], 'verb': [['pp.']]}]\n",
      "7 :  japtÄ ['jap'] [{'noun': ['nom. sg. f.'], 'verbform': ['\"SKTMW93.html#H_jap\"'], 'verb': [['pp.']]}]\n",
      "7 :  japte ['jap'] [{'noun': ['acc. du. f.', 'nom. du. f.', 'acc. du. n.', 'nom. du. n.', 'loc. sg. n.', 'loc. sg. m.'], 'verbform': ['\"SKTMW93.html#H_jap\"'], 'verb': [['pp.']]}]\n",
      "Analyzing  kavitvam\n",
      "0 :  kavitvam ['kavtva'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "0 :  kavi ['kav'] [{'compound': ['iic.']}]\n",
      "0 :  ka ['ka_1'] [{'compound': ['iic.']}]\n",
      "2 :  vi ['vÄ«_2'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "2 :  vi ['vÄ«_2'] [{'compound': ['iic.']}]\n",
      "4 :  tvam ['-tva'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "4 :  tvam ['yuá¹£mad'] [{'noun': ['nom. sg. *']}]\n",
      "Analyzing  karoti\n",
      "0 :  karoti ['ká¹\\x9b_1'] [{'verb': ['pr. [8] ac. sg. 3']}]\n",
      "Analyzing  nAtra\n",
      "0 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "1 :  atra ['atra_1'] [{'undetermined': ['adv.']}]\n",
      "Analyzing  sandehaḥ\n",
      "0 :  sandehas ['sandeha'] [{'noun': ['nom. sg. m.']}]\n",
      "0 :  san ['sat', 'as_1'] [{'noun': ['nom. sg. m.']}, {'verb': ['ppr. [2] ac.']}]\n",
      "3 :  dehas ['deha'] [{'noun': ['nom. sg. m.']}]\n"
     ]
    }
   ],
   "source": [
    "SeeSentence(sentenceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup python FilterGood.py 0 2 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 2 4 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 4 8 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 8 16 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 16 32 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 32 64 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 64 128 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 128 256 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 256 512 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 512 1024 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 1024 2048 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 2048 4096 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 4096 8192 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 8192 16384 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 16384 32768 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 32768 65536 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 65536 131072 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 131072 262144 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 262144 420000 ../TextSegmentation/Pickle_Files/ &\n",
      "nohup python FilterGood.py 0 2 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 2 4 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 4 8 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 8 16 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 16 32 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 32 64 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 64 128 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 128 256 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 256 512 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 512 1024 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 1024 2048 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 2048 4096 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 4096 8192 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 8192 16384 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 16384 32768 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 32768 65536 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 65536 131072 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 131072 262144 ../TextSegmentation/corrected_10to20/ &\n",
      "nohup python FilterGood.py 262144 420000 ../TextSegmentation/corrected_10to20/ &\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "start = 0\n",
    "finish = 0\n",
    "for i in range(0, int(math.log2(420000)) + 1):\n",
    "    start = finish\n",
    "    finish = min(math.pow(2, i+1), 420000)\n",
    "    print(\"nohup python FilterGood.py {0} {1} ../TextSegmentation/Pickle_Files/ &\".format(int(start), int(finish)))\n",
    "    \n",
    "    \n",
    "start = 0\n",
    "finish = 0\n",
    "for i in range(0, int(math.log2(420000)) + 1):\n",
    "    start = finish\n",
    "    finish = min(math.pow(2, i+1), 420000)\n",
    "    print(\"nohup python FilterGood.py {0} {1} ../TextSegmentation/corrected_10to20/ &\".format(int(start), int(finish)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pf1 = pd.read_csv('extras/pvbnhi@pf.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "df_pf2 = pd.read_csv('extras/pvbnhi@skt.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "df_pf3 = pd.read_csv('extras/pvbnhi@upd.csv', header=None, names=['file', 'miss', 'miss2'])\n",
    "\n",
    "# df_pf1['miss'] = df_pf1[df_pf1['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "df_pf1 = df_pf1[df_pf1['miss'] == '0']\n",
    "df_pf1['folder'] = '../TextSegmentation/Pickle_Files/'\n",
    "\n",
    "# df_pf2['miss'] = df_pf2[df_pf2['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "df_pf2 = df_pf2[df_pf2['miss'] == '0']\n",
    "df_pf2['folder'] = '../TextSegmentation/corrected_10to20/'\n",
    "\n",
    "# df_pf3['miss'] = df_pf3[df_pf3['miss'] != 'Incomplete chunk']['miss'].astype(int)\n",
    "df_pf3 = df_pf3[df_pf3['miss'] == '0']\n",
    "df_pf3['folder'] = '../TextSegmentation/Updated Pickles/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n"
     ]
    }
   ],
   "source": [
    "badis = []\n",
    "ids = df_pf1.index\n",
    "for x in range(len(ids)):\n",
    "    ix = ids[x]\n",
    "    f = '%d.p' % df_pf1.loc[ix, 'file']\n",
    "    p = '../TextSegmentation/Pickle_Files/%s' % f\n",
    "    skt, dcs = loadSentence(f, p)\n",
    "    if (x % 100 == 0):\n",
    "        print(x)\n",
    "    if(len(skt.chunk) != len(dcs.lemmas)):\n",
    "        df_pf1.loc[ix, 'miss'] = 'Length Mismatch'\n",
    "        badis.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n"
     ]
    }
   ],
   "source": [
    "ids = df_pf2.index\n",
    "for x in range(len(ids)):\n",
    "    ix = ids[x]\n",
    "    f = '%d.p' % df_pf2.loc[ix, 'file']\n",
    "    p = '../TextSegmentation/corrected_10to20/%s' % f\n",
    "    skt, dcs = loadSentence(f, p)\n",
    "    if (x % 100 == 0):\n",
    "        print(x)\n",
    "    if(len(skt.chunk) != len(dcs.lemmas)):\n",
    "        df_pf2.loc[ix, 'miss'] = 'Length Mismatch'\n",
    "        badis.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n"
     ]
    }
   ],
   "source": [
    "ids = df_pf3.index\n",
    "for x in range(len(ids)):\n",
    "    ix = ids[x]\n",
    "    f = '%d.p' % df_pf3.loc[ix, 'file']\n",
    "    p = '../TextSegmentation/Updated Pickles/%s' % f\n",
    "    skt, dcs = loadSentence(f, p)\n",
    "    if (x % 100 == 0):\n",
    "        print(x)\n",
    "    if(len(skt.chunk) != len(dcs.lemmas)):\n",
    "        df_pf3.loc[ix, 'miss'] = 'Length Mismatch'\n",
    "        badis.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pf1 = df_pf1[df_pf1['miss'] != 'Length Mismatch']\n",
    "df_pf2 = df_pf2[df_pf2['miss'] != 'Length Mismatch']\n",
    "df_pf3 = df_pf3[df_pf3['miss'] != 'Length Mismatch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  n\n",
      "Nothing done.\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  n\n",
      "Nothing done.\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  n\n",
      "Nothing done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49441"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [df_pf1, df_pf2, df_pf3]\n",
    "df_pf = pd.concat(frames)\n",
    "# %reset_selective df_pf1\n",
    "# %reset_selective df_pf2\n",
    "# %reset_selective df_pf3\n",
    "\n",
    "goodFileDict = {}\n",
    "for index, row in df_pf.iterrows():\n",
    "    goodFileDict['%d.p' % row['file']] = '%s%d.p' % (row['folder'], row['file'])\n",
    "len(goodFileDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(goodFileDict, open('mergedGood_v5.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = os.listdir('.temp/July_2_Morning_002/')\n",
    "\n",
    "allAcc = []\n",
    "for f in fs:\n",
    "    td = pickle.load(open('.temp/July_2_Morning_002/' + f, 'rb'))\n",
    "    [allAcc.append(ac) for ac in td.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allAcc = np.array(allAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17384828778025377"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(allAcc >= 100)/allAcc.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
